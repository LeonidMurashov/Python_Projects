{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRY\n",
    "top_k_categorical_accuracy\n",
    "https://stackoverflow.com/questions/47887533/keras-convolution-along-samples\n",
    "https://keras.io/layers/wrappers/#timedistributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This file differs from 'training_simple_single' by new data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "input_width = 160\n",
    "input_height = 100\n",
    "channels = 3\n",
    "class_number = 12\n",
    "data_path = \"D:\\\\Python\\\\Keras\\\\Wormax\\\\data_prepared\\\\\"\n",
    "model_name = 'models/worm_single_xceptiondelme.h5'\n",
    "look_forward = 1\n",
    "use_rotation = False\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras import layers, models, optimizers\n",
    "import keras.backend as K\n",
    "\n",
    "def actual_acc(y_true, y_pred):\n",
    "    return K.equal(K.argmax(y_pred), K.argmax(y_true))\n",
    "\n",
    "def define_model():\n",
    "    '''model = models.Sequential()\n",
    "        \n",
    "    model.add(layers.Convolution2D(64, (8, 8), strides=(4, 4), activation='relu',\n",
    "                            input_shape=(input_height, input_width, channels)))\n",
    "    model.add(layers.Convolution2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "    model.add(layers.Convolution2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dense(class_number, activation='softmax'))    '''\n",
    "    model = keras.applications.mobilenetv2.MobileNetV2(include_top=True, \n",
    "                                                       weights=None, \n",
    "                                                       input_tensor=None, \n",
    "                                                       input_shape=(input_height, input_width, channels), \n",
    "                                                       pooling=None, \n",
    "                                                       classes=12)\n",
    "\n",
    "    model.compile(optimizer=optimizers.Adam(lr=1e-3),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=[actual_acc])\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100, 160, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv2D)                  (None, 50, 80, 32)   864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn_Conv1 (BatchNormalization)   (None, 50, 80, 32)   128         Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_relu (ReLU)               (None, 50, 80, 32)   0           bn_Conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise (Depthw (None, 50, 80, 32)   288         Conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_BN (Bat (None, 50, 80, 32)   128         expanded_conv_depthwise[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_relu (R (None, 50, 80, 32)   0           expanded_conv_depthwise_BN[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project (Conv2D)  (None, 50, 80, 16)   512         expanded_conv_depthwise_relu[0][0\n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project_BN (Batch (None, 50, 80, 16)   64          expanded_conv_project[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand (Conv2D)         (None, 50, 80, 96)   1536        expanded_conv_project_BN[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_BN (BatchNormali (None, 50, 80, 96)   384         block_1_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_relu (ReLU)      (None, 50, 80, 96)   0           block_1_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise (DepthwiseCon (None, 25, 40, 96)   864         block_1_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_BN (BatchNorm (None, 25, 40, 96)   384         block_1_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_relu (ReLU)   (None, 25, 40, 96)   0           block_1_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project (Conv2D)        (None, 25, 40, 24)   2304        block_1_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project_BN (BatchNormal (None, 25, 40, 24)   96          block_1_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand (Conv2D)         (None, 25, 40, 144)  3456        block_1_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_BN (BatchNormali (None, 25, 40, 144)  576         block_2_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_relu (ReLU)      (None, 25, 40, 144)  0           block_2_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise (DepthwiseCon (None, 25, 40, 144)  1296        block_2_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_BN (BatchNorm (None, 25, 40, 144)  576         block_2_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_relu (ReLU)   (None, 25, 40, 144)  0           block_2_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project (Conv2D)        (None, 25, 40, 24)   3456        block_2_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project_BN (BatchNormal (None, 25, 40, 24)   96          block_2_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_add (Add)               (None, 25, 40, 24)   0           block_1_project_BN[0][0]         \n",
      "                                                                 block_2_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand (Conv2D)         (None, 25, 40, 144)  3456        block_2_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_BN (BatchNormali (None, 25, 40, 144)  576         block_3_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_relu (ReLU)      (None, 25, 40, 144)  0           block_3_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise (DepthwiseCon (None, 13, 20, 144)  1296        block_3_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_BN (BatchNorm (None, 13, 20, 144)  576         block_3_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_relu (ReLU)   (None, 13, 20, 144)  0           block_3_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project (Conv2D)        (None, 13, 20, 32)   4608        block_3_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project_BN (BatchNormal (None, 13, 20, 32)   128         block_3_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand (Conv2D)         (None, 13, 20, 192)  6144        block_3_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_BN (BatchNormali (None, 13, 20, 192)  768         block_4_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_relu (ReLU)      (None, 13, 20, 192)  0           block_4_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise (DepthwiseCon (None, 13, 20, 192)  1728        block_4_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_BN (BatchNorm (None, 13, 20, 192)  768         block_4_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_relu (ReLU)   (None, 13, 20, 192)  0           block_4_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project (Conv2D)        (None, 13, 20, 32)   6144        block_4_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project_BN (BatchNormal (None, 13, 20, 32)   128         block_4_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_add (Add)               (None, 13, 20, 32)   0           block_3_project_BN[0][0]         \n",
      "                                                                 block_4_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand (Conv2D)         (None, 13, 20, 192)  6144        block_4_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_BN (BatchNormali (None, 13, 20, 192)  768         block_5_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_relu (ReLU)      (None, 13, 20, 192)  0           block_5_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise (DepthwiseCon (None, 13, 20, 192)  1728        block_5_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_BN (BatchNorm (None, 13, 20, 192)  768         block_5_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_relu (ReLU)   (None, 13, 20, 192)  0           block_5_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project (Conv2D)        (None, 13, 20, 32)   6144        block_5_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project_BN (BatchNormal (None, 13, 20, 32)   128         block_5_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_5_add (Add)               (None, 13, 20, 32)   0           block_4_add[0][0]                \n",
      "                                                                 block_5_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand (Conv2D)         (None, 13, 20, 192)  6144        block_5_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_BN (BatchNormali (None, 13, 20, 192)  768         block_6_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_relu (ReLU)      (None, 13, 20, 192)  0           block_6_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise (DepthwiseCon (None, 7, 10, 192)   1728        block_6_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_BN (BatchNorm (None, 7, 10, 192)   768         block_6_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_relu (ReLU)   (None, 7, 10, 192)   0           block_6_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project (Conv2D)        (None, 7, 10, 64)    12288       block_6_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project_BN (BatchNormal (None, 7, 10, 64)    256         block_6_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand (Conv2D)         (None, 7, 10, 384)   24576       block_6_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_BN (BatchNormali (None, 7, 10, 384)   1536        block_7_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_relu (ReLU)      (None, 7, 10, 384)   0           block_7_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise (DepthwiseCon (None, 7, 10, 384)   3456        block_7_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_BN (BatchNorm (None, 7, 10, 384)   1536        block_7_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_relu (ReLU)   (None, 7, 10, 384)   0           block_7_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project (Conv2D)        (None, 7, 10, 64)    24576       block_7_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project_BN (BatchNormal (None, 7, 10, 64)    256         block_7_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_add (Add)               (None, 7, 10, 64)    0           block_6_project_BN[0][0]         \n",
      "                                                                 block_7_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand (Conv2D)         (None, 7, 10, 384)   24576       block_7_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_BN (BatchNormali (None, 7, 10, 384)   1536        block_8_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_relu (ReLU)      (None, 7, 10, 384)   0           block_8_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise (DepthwiseCon (None, 7, 10, 384)   3456        block_8_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_BN (BatchNorm (None, 7, 10, 384)   1536        block_8_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_relu (ReLU)   (None, 7, 10, 384)   0           block_8_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project (Conv2D)        (None, 7, 10, 64)    24576       block_8_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project_BN (BatchNormal (None, 7, 10, 64)    256         block_8_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_8_add (Add)               (None, 7, 10, 64)    0           block_7_add[0][0]                \n",
      "                                                                 block_8_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand (Conv2D)         (None, 7, 10, 384)   24576       block_8_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_BN (BatchNormali (None, 7, 10, 384)   1536        block_9_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_relu (ReLU)      (None, 7, 10, 384)   0           block_9_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise (DepthwiseCon (None, 7, 10, 384)   3456        block_9_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_BN (BatchNorm (None, 7, 10, 384)   1536        block_9_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_relu (ReLU)   (None, 7, 10, 384)   0           block_9_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project (Conv2D)        (None, 7, 10, 64)    24576       block_9_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project_BN (BatchNormal (None, 7, 10, 64)    256         block_9_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_9_add (Add)               (None, 7, 10, 64)    0           block_8_add[0][0]                \n",
      "                                                                 block_9_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand (Conv2D)        (None, 7, 10, 384)   24576       block_9_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_BN (BatchNormal (None, 7, 10, 384)   1536        block_10_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_relu (ReLU)     (None, 7, 10, 384)   0           block_10_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise (DepthwiseCo (None, 7, 10, 384)   3456        block_10_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_BN (BatchNor (None, 7, 10, 384)   1536        block_10_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_relu (ReLU)  (None, 7, 10, 384)   0           block_10_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project (Conv2D)       (None, 7, 10, 96)    36864       block_10_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project_BN (BatchNorma (None, 7, 10, 96)    384         block_10_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand (Conv2D)        (None, 7, 10, 576)   55296       block_10_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_BN (BatchNormal (None, 7, 10, 576)   2304        block_11_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_relu (ReLU)     (None, 7, 10, 576)   0           block_11_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise (DepthwiseCo (None, 7, 10, 576)   5184        block_11_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_BN (BatchNor (None, 7, 10, 576)   2304        block_11_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_relu (ReLU)  (None, 7, 10, 576)   0           block_11_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project (Conv2D)       (None, 7, 10, 96)    55296       block_11_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project_BN (BatchNorma (None, 7, 10, 96)    384         block_11_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_add (Add)              (None, 7, 10, 96)    0           block_10_project_BN[0][0]        \n",
      "                                                                 block_11_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand (Conv2D)        (None, 7, 10, 576)   55296       block_11_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_BN (BatchNormal (None, 7, 10, 576)   2304        block_12_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_relu (ReLU)     (None, 7, 10, 576)   0           block_12_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise (DepthwiseCo (None, 7, 10, 576)   5184        block_12_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_BN (BatchNor (None, 7, 10, 576)   2304        block_12_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_relu (ReLU)  (None, 7, 10, 576)   0           block_12_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project (Conv2D)       (None, 7, 10, 96)    55296       block_12_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project_BN (BatchNorma (None, 7, 10, 96)    384         block_12_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_12_add (Add)              (None, 7, 10, 96)    0           block_11_add[0][0]               \n",
      "                                                                 block_12_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand (Conv2D)        (None, 7, 10, 576)   55296       block_12_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_BN (BatchNormal (None, 7, 10, 576)   2304        block_13_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_relu (ReLU)     (None, 7, 10, 576)   0           block_13_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise (DepthwiseCo (None, 4, 5, 576)    5184        block_13_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_BN (BatchNor (None, 4, 5, 576)    2304        block_13_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_relu (ReLU)  (None, 4, 5, 576)    0           block_13_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project (Conv2D)       (None, 4, 5, 160)    92160       block_13_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project_BN (BatchNorma (None, 4, 5, 160)    640         block_13_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand (Conv2D)        (None, 4, 5, 960)    153600      block_13_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_BN (BatchNormal (None, 4, 5, 960)    3840        block_14_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_relu (ReLU)     (None, 4, 5, 960)    0           block_14_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise (DepthwiseCo (None, 4, 5, 960)    8640        block_14_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_BN (BatchNor (None, 4, 5, 960)    3840        block_14_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_relu (ReLU)  (None, 4, 5, 960)    0           block_14_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project (Conv2D)       (None, 4, 5, 160)    153600      block_14_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project_BN (BatchNorma (None, 4, 5, 160)    640         block_14_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_add (Add)              (None, 4, 5, 160)    0           block_13_project_BN[0][0]        \n",
      "                                                                 block_14_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand (Conv2D)        (None, 4, 5, 960)    153600      block_14_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_BN (BatchNormal (None, 4, 5, 960)    3840        block_15_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_relu (ReLU)     (None, 4, 5, 960)    0           block_15_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise (DepthwiseCo (None, 4, 5, 960)    8640        block_15_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_BN (BatchNor (None, 4, 5, 960)    3840        block_15_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_relu (ReLU)  (None, 4, 5, 960)    0           block_15_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project (Conv2D)       (None, 4, 5, 160)    153600      block_15_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project_BN (BatchNorma (None, 4, 5, 160)    640         block_15_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_15_add (Add)              (None, 4, 5, 160)    0           block_14_add[0][0]               \n",
      "                                                                 block_15_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand (Conv2D)        (None, 4, 5, 960)    153600      block_15_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_BN (BatchNormal (None, 4, 5, 960)    3840        block_16_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_relu (ReLU)     (None, 4, 5, 960)    0           block_16_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise (DepthwiseCo (None, 4, 5, 960)    8640        block_16_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_BN (BatchNor (None, 4, 5, 960)    3840        block_16_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_relu (ReLU)  (None, 4, 5, 960)    0           block_16_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project (Conv2D)       (None, 4, 5, 320)    307200      block_16_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project_BN (BatchNorma (None, 4, 5, 320)    1280        block_16_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1 (Conv2D)                 (None, 4, 5, 1280)   409600      block_16_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1_bn (BatchNormalization)  (None, 4, 5, 1280)   5120        Conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_relu (ReLU)                 (None, 4, 5, 1280)   0           Conv_1_bn[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 1280)         0           out_relu[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Logits (Dense)                  (None, 12)           15372       global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 2,273,356\n",
      "Trainable params: 2,239,244\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = define_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# little prepocessing\n",
    "from math import atan2, pi\n",
    "import cv2\n",
    "\n",
    "def get_angle(x, y):\n",
    "    return atan2(y, x)\n",
    "\n",
    "def get_direction(x, y, n_classes = 12):\n",
    "    return round(get_angle(x, y)/2/pi*n_classes)%n_classes\n",
    "\n",
    "# Rotating frame and direction counter-clockwise\n",
    "def get_rotated_frame(img, rotation, target_direction, n_classes):\n",
    "    if rotation != 0:\n",
    "        rows, cols = len(img),len(img[0])\n",
    "        rot_M = cv2.getRotationMatrix2D((cols/2, rows/2), rotation*360/n_classes, 1)\n",
    "        img = cv2.warpAffine(img, rot_M, (cols, rows))\n",
    "        target = (target_direction - rotation)%n_classes\n",
    "    return img, target_direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from functools import reduce\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Training and validation\n",
    "data_ratio = 0.7\n",
    "\n",
    "def generator(data_dir, n_classes, role, shuffle=True, batch_size=128):\n",
    "    \n",
    "    listdir = []\n",
    "    listdir = filter(lambda x: os.path.isfile, os.listdir(data_dir))\n",
    "    listdir = np.array(list(listdir))\n",
    "    random.shuffle(listdir)\n",
    "    \n",
    "    #print('Found {} files for {}'.format(len(listdir), role))\n",
    "    \n",
    "    file_i = 0\n",
    "    while 1:\n",
    "        arr = np.load(data_dir + listdir[file_i])        \n",
    "        file_i = (file_i+1) if file_i+1<len(listdir) else 0\n",
    "        \n",
    "        # Expanding blocks\n",
    "        data = []        \n",
    "        for i in arr:\n",
    "            for j in i:\n",
    "                data.append(j)\n",
    "        data = np.array(data)\n",
    "\n",
    "        \n",
    "        if role == 'train':\n",
    "            data = data[:int(round(len(data)*data_ratio))]\n",
    "        elif role == 'validation':\n",
    "            data = data[int(round(len(data)*data_ratio)):]\n",
    "        else:\n",
    "            raise 'bad role parameter'\n",
    "        \n",
    "        # Generating y\n",
    "        data_targets = np.zeros((len(data)-look_forward))\n",
    "        for i in range(len(data_targets)):\n",
    "            data_targets[i] = get_direction(*data[i+look_forward][1][:2])\n",
    "            #data_targets[i] = np.array(to_categorical(get_direction(*data[i+look_forward][1][:2]), num_classes=num_classes))\n",
    "        data = data[:len(data)-look_forward]\n",
    "        \n",
    "        # Only X\n",
    "        data_features = np.zeros((len(data), input_height, input_width, channels))\n",
    "        for i in range(len(data)):\n",
    "            data_features[i] = data[i][0][0]\n",
    "        \n",
    "        indexes = np.arange(len(data_features)-look_forward)        \n",
    "        if shuffle:\n",
    "            np.random.shuffle(indexes)\n",
    "        \n",
    "        # Rotating cycle(no rotation for validation)\n",
    "        rotate_times = n_classes if role == 'train' and use_rotation else 1\n",
    "        for rot_i in range(rotate_times):\n",
    "            # Batches forming cycle\n",
    "            for i in range(0, len(data)-look_forward-batch_size, batch_size):\n",
    "                samples = data_features.take(indexes[i:i+batch_size], axis=0)\n",
    "                targets = data_targets.take(indexes[i:i+batch_size], axis=0)\n",
    "                samples_rot = np.zeros_like(samples)\n",
    "                targets_rot = np.zeros((batch_size, n_classes))\n",
    "                \n",
    "                # Rotating\n",
    "                for j in range(batch_size):\n",
    "                    if use_rotation:\n",
    "                        rotation = (rot_i+j+i)%n_classes\n",
    "                        img, target = get_rotated_frame(samples[j], rotation, targets[j], n_classes)\n",
    "                    else:\n",
    "                        img, target = samples[j], targets[j]\n",
    "                    samples_rot[j] = img\n",
    "                    targets_rot[j] = to_categorical(target, num_classes=n_classes)\n",
    "                    \n",
    "                # Normilize\n",
    "                samples_rot = samples_rot / 255\n",
    "                yield samples_rot, targets_rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 100, 160, 3)\n",
      "(16, 12)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_generator = generator(data_path, class_number, 'train', batch_size=16)\n",
    "validation_generator = generator(data_path, class_number, 'validation', batch_size=32)\n",
    "\n",
    "print(next(train_generator)[0].shape)\n",
    "print(next(train_generator)[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count class instances count for balancing\n",
    "if False:\n",
    "    i = 0\n",
    "    classes = np.zeros((class_number))\n",
    "    for samples, targets in generator(data_path, class_number, 'train', batch_size=1024):\n",
    "        for j in targets:\n",
    "            classes += j\n",
    "        i += 1\n",
    "        if i == 100:\n",
    "            break\n",
    "    print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "### tensorboard --logdir=D:\\Python\\Keras\\Wormax\\log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "    2) tune look_forward\n",
    "    3) change target to angle\n",
    "    4) fight overfitting on adam\n",
    "    5) Take away poolingb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am trying to solve classification task. Basically, teaching game bot on human actions.  \n",
    "So there are more than 1 right answers for X. But my dataset contains specific actions.\n",
    "\n",
    "Assume the desirable answer is:`[0, 0.7, 0, 0.7, 0.1]`  \n",
    "But dataset contains one-hots like: `[0, 0, 0, 1, 0]`\n",
    "\n",
    "So my idea is to implement Q-state neural network with 2 inputs.  \n",
    "Q(S,a) - where, S - current state, a - action, result is one number(desire to choose this action).  \n",
    "And then I can feed to it my X->y dataset.\n",
    "\n",
    "I am struggling with initializing model. Default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epoch = 0\n",
    "if not True:\n",
    "    model.load_weights(model_name)\n",
    "    initial_epoch = 176"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "100/100 [==============================] - 28s 284ms/step - loss: 3.0791 - actual_acc: 0.1444 - val_loss: 5.6225 - val_actual_acc: 0.0705\n",
      "Epoch 2/500\n",
      "100/100 [==============================] - 22s 223ms/step - loss: 2.9786 - actual_acc: 0.1694 - val_loss: 7.6119 - val_actual_acc: 0.0734\n",
      "Epoch 3/500\n",
      "100/100 [==============================] - 17s 175ms/step - loss: 2.8070 - actual_acc: 0.2081 - val_loss: 7.2331 - val_actual_acc: 0.1497\n",
      "Epoch 4/500\n",
      "100/100 [==============================] - 22s 222ms/step - loss: 2.6245 - actual_acc: 0.2781 - val_loss: 9.9215 - val_actual_acc: 0.1235\n",
      "Epoch 5/500\n",
      "100/100 [==============================] - 21s 206ms/step - loss: 2.4907 - actual_acc: 0.3031 - val_loss: 9.6368 - val_actual_acc: 0.0843\n",
      "Epoch 6/500\n",
      "100/100 [==============================] - 20s 196ms/step - loss: 2.8118 - actual_acc: 0.2112 - val_loss: 5.0674 - val_actual_acc: 0.0843\n",
      "Epoch 7/500\n",
      "100/100 [==============================] - 18s 175ms/step - loss: 2.5093 - actual_acc: 0.2913 - val_loss: 7.9152 - val_actual_acc: 0.0552\n",
      "Epoch 8/500\n",
      "100/100 [==============================] - 20s 203ms/step - loss: 2.3442 - actual_acc: 0.3444 - val_loss: 6.1584 - val_actual_acc: 0.0938\n",
      "Epoch 9/500\n",
      "100/100 [==============================] - 25s 247ms/step - loss: 2.3768 - actual_acc: 0.3431 - val_loss: 5.8939 - val_actual_acc: 0.0916\n",
      "Epoch 10/500\n",
      "100/100 [==============================] - 18s 181ms/step - loss: 2.5091 - actual_acc: 0.3038 - val_loss: 7.6540 - val_actual_acc: 0.0908\n",
      "Epoch 11/500\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 2.2142 - actual_acc: 0.3569 - val_loss: 10.1094 - val_actual_acc: 0.0770\n",
      "Epoch 12/500\n",
      "100/100 [==============================] - 20s 204ms/step - loss: 2.0072 - actual_acc: 0.4338 - val_loss: 8.9589 - val_actual_acc: 0.0531\n",
      "Epoch 13/500\n",
      "100/100 [==============================] - 24s 245ms/step - loss: 2.0641 - actual_acc: 0.4200 - val_loss: 10.9021 - val_actual_acc: 0.0785\n",
      "Epoch 14/500\n",
      "100/100 [==============================] - 20s 195ms/step - loss: 2.3230 - actual_acc: 0.3438 - val_loss: 10.7148 - val_actual_acc: 0.0792\n",
      "Epoch 15/500\n",
      "100/100 [==============================] - 25s 248ms/step - loss: 1.9803 - actual_acc: 0.4269 - val_loss: 13.7520 - val_actual_acc: 0.0676\n",
      "Epoch 16/500\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 1.8474 - actual_acc: 0.4537 - val_loss: 11.6184 - val_actual_acc: 0.0952\n",
      "Epoch 17/500\n",
      "100/100 [==============================] - 33s 328ms/step - loss: 2.0920 - actual_acc: 0.3994 - val_loss: 10.7418 - val_actual_acc: 0.0749\n",
      "Epoch 18/500\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 1.9988 - actual_acc: 0.4331 - val_loss: 6.2222 - val_actual_acc: 0.1185\n",
      "Epoch 19/500\n",
      "100/100 [==============================] - 23s 226ms/step - loss: 1.7098 - actual_acc: 0.5112 - val_loss: 6.3592 - val_actual_acc: 0.1083\n",
      "Epoch 20/500\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 1.5976 - actual_acc: 0.5225 - val_loss: 8.0969 - val_actual_acc: 0.1192\n",
      "Epoch 21/500\n",
      "100/100 [==============================] - 29s 286ms/step - loss: 1.9478 - actual_acc: 0.4487 - val_loss: 12.2955 - val_actual_acc: 0.0596\n",
      "Epoch 22/500\n",
      "100/100 [==============================] - 17s 173ms/step - loss: 2.0735 - actual_acc: 0.3775 - val_loss: 9.7923 - val_actual_acc: 0.0996\n",
      "Epoch 23/500\n",
      "100/100 [==============================] - 23s 232ms/step - loss: 1.7489 - actual_acc: 0.4806 - val_loss: 11.1097 - val_actual_acc: 0.1039\n",
      "Epoch 24/500\n",
      "100/100 [==============================] - 23s 231ms/step - loss: 2.0477 - actual_acc: 0.4125 - val_loss: 12.5740 - val_actual_acc: 0.0814\n",
      "Epoch 25/500\n",
      "100/100 [==============================] - 25s 246ms/step - loss: 2.0042 - actual_acc: 0.4037 - val_loss: 11.9880 - val_actual_acc: 0.0988\n",
      "Epoch 26/500\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 1.8321 - actual_acc: 0.4550 - val_loss: 6.7765 - val_actual_acc: 0.1097\n",
      "Epoch 27/500\n",
      "100/100 [==============================] - 24s 242ms/step - loss: 1.6148 - actual_acc: 0.5244 - val_loss: 8.0634 - val_actual_acc: 0.1076\n",
      "Epoch 28/500\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 2.0563 - actual_acc: 0.3975 - val_loss: 4.8494 - val_actual_acc: 0.1359\n",
      "Epoch 29/500\n",
      "100/100 [==============================] - 24s 242ms/step - loss: 1.9546 - actual_acc: 0.4387 - val_loss: 6.1617 - val_actual_acc: 0.1192\n",
      "Epoch 30/500\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 1.5822 - actual_acc: 0.5438 - val_loss: 7.7200 - val_actual_acc: 0.1388\n",
      "Epoch 31/500\n",
      "100/100 [==============================] - 22s 223ms/step - loss: 1.5308 - actual_acc: 0.5325 - val_loss: 7.9623 - val_actual_acc: 0.1257\n",
      "Epoch 32/500\n",
      "100/100 [==============================] - 27s 269ms/step - loss: 1.9983 - actual_acc: 0.4188 - val_loss: 8.6773 - val_actual_acc: 0.1221\n",
      "Epoch 33/500\n",
      "100/100 [==============================] - 17s 175ms/step - loss: 1.7467 - actual_acc: 0.4813 - val_loss: 7.7223 - val_actual_acc: 0.0836\n",
      "Epoch 34/500\n",
      "100/100 [==============================] - 24s 244ms/step - loss: 1.4670 - actual_acc: 0.5713 - val_loss: 5.9457 - val_actual_acc: 0.1228\n",
      "Epoch 35/500\n",
      "100/100 [==============================] - 24s 245ms/step - loss: 2.1007 - actual_acc: 0.4075 - val_loss: 7.7715 - val_actual_acc: 0.0741\n",
      "Epoch 36/500\n",
      "100/100 [==============================] - 18s 177ms/step - loss: 1.8646 - actual_acc: 0.4363 - val_loss: 8.2961 - val_actual_acc: 0.1105\n",
      "Epoch 37/500\n",
      "100/100 [==============================] - 21s 213ms/step - loss: 1.6145 - actual_acc: 0.5125 - val_loss: 7.4275 - val_actual_acc: 0.1352\n",
      "Epoch 38/500\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 1.5366 - actual_acc: 0.5281 - val_loss: 7.9606 - val_actual_acc: 0.1272\n",
      "Epoch 39/500\n",
      "100/100 [==============================] - 35s 348ms/step - loss: 2.2594 - actual_acc: 0.3387 - val_loss: 9.0000 - val_actual_acc: 0.1265\n",
      "Epoch 40/500\n",
      "100/100 [==============================] - 27s 274ms/step - loss: 1.9983 - actual_acc: 0.3944 - val_loss: 8.3682 - val_actual_acc: 0.1359\n",
      "Epoch 41/500\n",
      "100/100 [==============================] - 17s 172ms/step - loss: 1.7824 - actual_acc: 0.4662 - val_loss: 5.7962 - val_actual_acc: 0.1134\n",
      "Epoch 42/500\n",
      "100/100 [==============================] - 24s 239ms/step - loss: 1.7353 - actual_acc: 0.5031 - val_loss: 6.7017 - val_actual_acc: 0.0952\n",
      "Epoch 43/500\n",
      "100/100 [==============================] - 21s 206ms/step - loss: 1.9550 - actual_acc: 0.4037 - val_loss: 8.9786 - val_actual_acc: 0.1047\n",
      "Epoch 44/500\n",
      "100/100 [==============================] - 25s 246ms/step - loss: 1.5236 - actual_acc: 0.5463 - val_loss: 6.2022 - val_actual_acc: 0.1228\n",
      "Epoch 45/500\n",
      "100/100 [==============================] - 17s 175ms/step - loss: 1.3476 - actual_acc: 0.5913 - val_loss: 8.4451 - val_actual_acc: 0.0996\n",
      "Epoch 46/500\n",
      "100/100 [==============================] - 30s 302ms/step - loss: 1.1925 - actual_acc: 0.6381 - val_loss: 9.8033 - val_actual_acc: 0.0698\n",
      "Epoch 47/500\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 2.1594 - actual_acc: 0.3912 - val_loss: 9.0074 - val_actual_acc: 0.1308\n",
      "Epoch 48/500\n",
      "100/100 [==============================] - 24s 242ms/step - loss: 1.7201 - actual_acc: 0.4969 - val_loss: 8.7388 - val_actual_acc: 0.1286\n",
      "Epoch 49/500\n",
      "100/100 [==============================] - 17s 175ms/step - loss: 1.4934 - actual_acc: 0.5556 - val_loss: 8.3538 - val_actual_acc: 0.1432\n",
      "Epoch 50/500\n",
      "100/100 [==============================] - 23s 235ms/step - loss: 1.3922 - actual_acc: 0.5969 - val_loss: 10.8870 - val_actual_acc: 0.0741\n",
      "Epoch 51/500\n",
      "100/100 [==============================] - 22s 217ms/step - loss: 1.9114 - actual_acc: 0.4487 - val_loss: 10.7908 - val_actual_acc: 0.0836\n",
      "Epoch 52/500\n",
      "100/100 [==============================] - 25s 254ms/step - loss: 2.0174 - actual_acc: 0.3956 - val_loss: 10.0041 - val_actual_acc: 0.1054\n",
      "Epoch 53/500\n",
      "100/100 [==============================] - 17s 175ms/step - loss: 1.7921 - actual_acc: 0.4900 - val_loss: 8.3509 - val_actual_acc: 0.1010\n",
      "Epoch 54/500\n",
      "100/100 [==============================] - 24s 240ms/step - loss: 1.4972 - actual_acc: 0.5513 - val_loss: 4.6661 - val_actual_acc: 0.1570\n",
      "Epoch 55/500\n",
      "100/100 [==============================] - 25s 245ms/step - loss: 2.0080 - actual_acc: 0.4150 - val_loss: 6.2333 - val_actual_acc: 0.0945\n",
      "Epoch 56/500\n",
      "100/100 [==============================] - 24s 241ms/step - loss: 1.7991 - actual_acc: 0.4694 - val_loss: 4.7761 - val_actual_acc: 0.2137\n",
      "Epoch 57/500\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 1.5822 - actual_acc: 0.5281 - val_loss: 7.2123 - val_actual_acc: 0.0836\n",
      "Epoch 58/500\n",
      "100/100 [==============================] - 24s 243ms/step - loss: 1.4268 - actual_acc: 0.5844 - val_loss: 8.4623 - val_actual_acc: 0.1119\n",
      "Epoch 59/500\n",
      "100/100 [==============================] - 24s 243ms/step - loss: 2.0000 - actual_acc: 0.4400 - val_loss: 4.5298 - val_actual_acc: 0.1017\n",
      "Epoch 60/500\n",
      "100/100 [==============================] - 25s 255ms/step - loss: 1.7282 - actual_acc: 0.4975 - val_loss: 4.5883 - val_actual_acc: 0.1432\n",
      "Epoch 61/500\n",
      "100/100 [==============================] - 17s 175ms/step - loss: 1.5124 - actual_acc: 0.5619 - val_loss: 6.0543 - val_actual_acc: 0.1352\n",
      "Epoch 62/500\n",
      "100/100 [==============================] - 31s 311ms/step - loss: 1.3116 - actual_acc: 0.6131 - val_loss: 6.9954 - val_actual_acc: 0.1119\n",
      "Epoch 63/500\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 2.4292 - actual_acc: 0.3244 - val_loss: 8.7757 - val_actual_acc: 0.0698\n",
      "Epoch 64/500\n",
      "100/100 [==============================] - 24s 242ms/step - loss: 2.0504 - actual_acc: 0.4269 - val_loss: 8.3195 - val_actual_acc: 0.1090\n",
      "Epoch 65/500\n",
      "100/100 [==============================] - 17s 175ms/step - loss: 1.7540 - actual_acc: 0.4944 - val_loss: 4.2419 - val_actual_acc: 0.1562\n",
      "Epoch 66/500\n",
      "100/100 [==============================] - 22s 222ms/step - loss: 1.6274 - actual_acc: 0.5238 - val_loss: 4.8648 - val_actual_acc: 0.1374\n",
      "Epoch 67/500\n",
      "100/100 [==============================] - 24s 239ms/step - loss: 1.9781 - actual_acc: 0.4269 - val_loss: 3.9688 - val_actual_acc: 0.1744\n",
      "Epoch 68/500\n",
      "100/100 [==============================] - 18s 180ms/step - loss: 1.5760 - actual_acc: 0.5238 - val_loss: 4.3047 - val_actual_acc: 0.1323\n",
      "Epoch 69/500\n",
      "100/100 [==============================] - 25s 246ms/step - loss: 1.3803 - actual_acc: 0.5850 - val_loss: 4.2107 - val_actual_acc: 0.1490\n",
      "Epoch 70/500\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 1.3940 - actual_acc: 0.6081 - val_loss: 5.9351 - val_actual_acc: 0.0785\n",
      "Epoch 71/500\n",
      "100/100 [==============================] - 24s 244ms/step - loss: 2.0972 - actual_acc: 0.3694 - val_loss: 4.6423 - val_actual_acc: 0.1294\n",
      "Epoch 72/500\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 1.7400 - actual_acc: 0.4775 - val_loss: 3.6520 - val_actual_acc: 0.1562\n",
      "Epoch 73/500\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 1.5330 - actual_acc: 0.5413 - val_loss: 4.6739 - val_actual_acc: 0.1265\n",
      "Epoch 74/500\n",
      "100/100 [==============================] - 26s 256ms/step - loss: 1.5142 - actual_acc: 0.5588 - val_loss: 4.2215 - val_actual_acc: 0.1301\n",
      "Epoch 75/500\n",
      "100/100 [==============================] - 23s 228ms/step - loss: 1.9643 - actual_acc: 0.4119 - val_loss: 3.7593 - val_actual_acc: 0.1962\n",
      "Epoch 76/500\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 1.5872 - actual_acc: 0.5238 - val_loss: 3.1858 - val_actual_acc: 0.2224\n",
      "Epoch 77/500\n",
      "100/100 [==============================] - 20s 198ms/step - loss: 1.3679 - actual_acc: 0.5837 - val_loss: 4.0581 - val_actual_acc: 0.1686\n",
      "Epoch 78/500\n",
      "100/100 [==============================] - 25s 255ms/step - loss: 1.6053 - actual_acc: 0.5262 - val_loss: 4.1672 - val_actual_acc: 0.1461\n",
      "Epoch 79/500\n",
      "100/100 [==============================] - 20s 204ms/step - loss: 1.9530 - actual_acc: 0.4375 - val_loss: 3.9773 - val_actual_acc: 0.1584\n",
      "Epoch 80/500\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 1.5439 - actual_acc: 0.5375 - val_loss: 4.8203 - val_actual_acc: 0.1555\n",
      "Epoch 81/500\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 1.3895 - actual_acc: 0.5837 - val_loss: 6.2916 - val_actual_acc: 0.0923\n",
      "Epoch 82/500\n",
      "100/100 [==============================] - 27s 271ms/step - loss: 1.7169 - actual_acc: 0.5069 - val_loss: 5.5588 - val_actual_acc: 0.1185\n",
      "Epoch 83/500\n",
      "100/100 [==============================] - 19s 188ms/step - loss: 2.0713 - actual_acc: 0.3787 - val_loss: 5.4908 - val_actual_acc: 0.1061\n",
      "Epoch 84/500\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 1.7068 - actual_acc: 0.4944 - val_loss: 4.1332 - val_actual_acc: 0.1744\n",
      "Epoch 85/500\n",
      "100/100 [==============================] - 21s 214ms/step - loss: 1.4855 - actual_acc: 0.5475 - val_loss: 4.8292 - val_actual_acc: 0.1381\n",
      "Epoch 86/500\n",
      "100/100 [==============================] - 34s 337ms/step - loss: 1.7158 - actual_acc: 0.5000 - val_loss: 5.0760 - val_actual_acc: 0.1679\n",
      "Epoch 87/500\n",
      "100/100 [==============================] - 17s 173ms/step - loss: 1.9563 - actual_acc: 0.4200 - val_loss: 5.2967 - val_actual_acc: 0.1453\n",
      "Epoch 88/500\n",
      "100/100 [==============================] - 25s 253ms/step - loss: 1.6117 - actual_acc: 0.5044 - val_loss: 4.3308 - val_actual_acc: 0.1453\n",
      "Epoch 89/500\n",
      "100/100 [==============================] - 18s 177ms/step - loss: 1.4441 - actual_acc: 0.5588 - val_loss: 4.5637 - val_actual_acc: 0.1613\n",
      "Epoch 90/500\n",
      "100/100 [==============================] - 33s 333ms/step - loss: 1.8478 - actual_acc: 0.4556 - val_loss: 3.7843 - val_actual_acc: 0.1330\n",
      "Epoch 91/500\n",
      "100/100 [==============================] - 17s 173ms/step - loss: 2.0384 - actual_acc: 0.3925 - val_loss: 2.8503 - val_actual_acc: 0.2231\n",
      "Epoch 92/500\n",
      "100/100 [==============================] - 25s 252ms/step - loss: 1.7150 - actual_acc: 0.4769 - val_loss: 3.1770 - val_actual_acc: 0.1977\n",
      "Epoch 93/500\n",
      "100/100 [==============================] - 17s 175ms/step - loss: 1.5125 - actual_acc: 0.5519 - val_loss: 3.4046 - val_actual_acc: 0.2376\n",
      "Epoch 94/500\n",
      "100/100 [==============================] - 32s 320ms/step - loss: 1.9291 - actual_acc: 0.4325 - val_loss: 5.7226 - val_actual_acc: 0.1395\n",
      "Epoch 95/500\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 1.7350 - actual_acc: 0.4556 - val_loss: 4.8995 - val_actual_acc: 0.1737\n",
      "Epoch 96/500\n",
      "100/100 [==============================] - 24s 241ms/step - loss: 1.4689 - actual_acc: 0.5413 - val_loss: 3.7199 - val_actual_acc: 0.1701\n",
      "Epoch 97/500\n",
      "100/100 [==============================] - 17s 172ms/step - loss: 1.3026 - actual_acc: 0.6106 - val_loss: 3.4557 - val_actual_acc: 0.2144\n",
      "Epoch 98/500\n",
      "100/100 [==============================] - 33s 332ms/step - loss: 2.0864 - actual_acc: 0.3844 - val_loss: 3.0493 - val_actual_acc: 0.2260\n",
      "Epoch 99/500\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 1.6935 - actual_acc: 0.4819 - val_loss: 3.4583 - val_actual_acc: 0.1926\n",
      "Epoch 100/500\n",
      "100/100 [==============================] - 26s 262ms/step - loss: 1.4285 - actual_acc: 0.5687 - val_loss: 3.6032 - val_actual_acc: 0.1708\n",
      "Epoch 101/500\n",
      "100/100 [==============================] - 26s 263ms/step - loss: 1.3639 - actual_acc: 0.5906 - val_loss: 3.8413 - val_actual_acc: 0.1831\n",
      "Epoch 102/500\n",
      "100/100 [==============================] - 25s 248ms/step - loss: 2.2146 - actual_acc: 0.3663 - val_loss: 3.5785 - val_actual_acc: 0.1570\n",
      "Epoch 103/500\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 1.6591 - actual_acc: 0.5175 - val_loss: 3.6194 - val_actual_acc: 0.1890\n",
      "Epoch 104/500\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 1.5484 - actual_acc: 0.5294 - val_loss: 3.2301 - val_actual_acc: 0.1824\n",
      "Epoch 105/500\n",
      "100/100 [==============================] - 31s 307ms/step - loss: 1.3559 - actual_acc: 0.6050 - val_loss: 3.7586 - val_actual_acc: 0.1097\n",
      "Epoch 106/500\n",
      "100/100 [==============================] - 18s 178ms/step - loss: 2.0089 - actual_acc: 0.4194 - val_loss: 4.9716 - val_actual_acc: 0.1265\n",
      "Epoch 107/500\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 1.6028 - actual_acc: 0.5294 - val_loss: 3.9146 - val_actual_acc: 0.1352\n",
      "Epoch 108/500\n",
      "100/100 [==============================] - 17s 173ms/step - loss: 1.2806 - actual_acc: 0.6156 - val_loss: 4.2616 - val_actual_acc: 0.1424\n",
      "Epoch 109/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 33s 331ms/step - loss: 1.3155 - actual_acc: 0.6144 - val_loss: 4.5244 - val_actual_acc: 0.1003\n",
      "Epoch 110/500\n",
      "100/100 [==============================] - 18s 177ms/step - loss: 2.0050 - actual_acc: 0.3994 - val_loss: 4.4326 - val_actual_acc: 0.1483\n",
      "Epoch 111/500\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 1.5623 - actual_acc: 0.5369 - val_loss: 3.7784 - val_actual_acc: 0.1374\n",
      "Epoch 112/500\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 1.3301 - actual_acc: 0.6213 - val_loss: 3.1517 - val_actual_acc: 0.2180\n",
      "Epoch 113/500\n",
      "100/100 [==============================] - 32s 319ms/step - loss: 1.3910 - actual_acc: 0.6019 - val_loss: 2.7903 - val_actual_acc: 0.2166\n",
      "Epoch 114/500\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 1.9472 - actual_acc: 0.4394 - val_loss: 3.0263 - val_actual_acc: 0.2144\n",
      "Epoch 115/500\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 1.4938 - actual_acc: 0.5588 - val_loss: 3.8904 - val_actual_acc: 0.1686\n",
      "Epoch 116/500\n",
      "100/100 [==============================] - 19s 188ms/step - loss: 1.2337 - actual_acc: 0.6400 - val_loss: 3.5222 - val_actual_acc: 0.2347\n",
      "Epoch 117/500\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 2.1459 - actual_acc: 0.3906 - val_loss: 4.0738 - val_actual_acc: 0.1664\n",
      "Epoch 118/500\n",
      "100/100 [==============================] - 18s 177ms/step - loss: 1.6035 - actual_acc: 0.5231 - val_loss: 3.1343 - val_actual_acc: 0.1679\n",
      "Epoch 119/500\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 1.3584 - actual_acc: 0.5869 - val_loss: 4.4315 - val_actual_acc: 0.1061\n",
      "Epoch 120/500\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 1.9567 - actual_acc: 0.4269 - val_loss: 5.8837 - val_actual_acc: 0.0719\n",
      "Epoch 121/500\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 1.9052 - actual_acc: 0.4400 - val_loss: 3.7105 - val_actual_acc: 0.1541\n",
      "Epoch 122/500\n",
      "100/100 [==============================] - 18s 177ms/step - loss: 1.5015 - actual_acc: 0.5381 - val_loss: 3.5784 - val_actual_acc: 0.1621\n",
      "Epoch 123/500\n",
      "100/100 [==============================] - 24s 243ms/step - loss: 1.3607 - actual_acc: 0.5931 - val_loss: 4.5772 - val_actual_acc: 0.1475\n",
      "Epoch 124/500\n",
      "100/100 [==============================] - 26s 261ms/step - loss: 2.1006 - actual_acc: 0.4150 - val_loss: 4.1887 - val_actual_acc: 0.1788\n",
      "Epoch 125/500\n",
      "100/100 [==============================] - 25s 247ms/step - loss: 1.6460 - actual_acc: 0.5269 - val_loss: 3.3577 - val_actual_acc: 0.1969\n",
      "Epoch 126/500\n",
      "100/100 [==============================] - 18s 177ms/step - loss: 1.3900 - actual_acc: 0.5794 - val_loss: 3.6735 - val_actual_acc: 0.1468\n",
      "Epoch 127/500\n",
      "100/100 [==============================] - 25s 247ms/step - loss: 1.2061 - actual_acc: 0.6625 - val_loss: 3.8670 - val_actual_acc: 0.1730\n",
      "Epoch 128/500\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 2.0119 - actual_acc: 0.4406 - val_loss: 3.2262 - val_actual_acc: 0.1868\n",
      "Epoch 129/500\n",
      "100/100 [==============================] - 25s 252ms/step - loss: 1.7074 - actual_acc: 0.5169 - val_loss: 3.3678 - val_actual_acc: 0.1737\n",
      "Epoch 130/500\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 1.4566 - actual_acc: 0.5594 - val_loss: 3.4891 - val_actual_acc: 0.1715\n",
      "Epoch 131/500\n",
      "100/100 [==============================] - 25s 253ms/step - loss: 1.3081 - actual_acc: 0.6012 - val_loss: 2.6289 - val_actual_acc: 0.2093\n",
      "Epoch 132/500\n",
      "100/100 [==============================] - 26s 262ms/step - loss: 1.9869 - actual_acc: 0.4431 - val_loss: 3.5539 - val_actual_acc: 0.1940\n",
      "Epoch 133/500\n",
      "100/100 [==============================] - 26s 262ms/step - loss: 1.6842 - actual_acc: 0.5006 - val_loss: 2.7336 - val_actual_acc: 0.2667\n",
      "Epoch 134/500\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 1.3866 - actual_acc: 0.5894 - val_loss: 2.8317 - val_actual_acc: 0.2544\n",
      "Epoch 135/500\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 1.2549 - actual_acc: 0.6206 - val_loss: 2.9592 - val_actual_acc: 0.2173\n",
      "Epoch 136/500\n",
      "100/100 [==============================] - 26s 255ms/step - loss: 1.9896 - actual_acc: 0.4350 - val_loss: 2.9387 - val_actual_acc: 0.2057\n",
      "Epoch 137/500\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 1.5862 - actual_acc: 0.5112 - val_loss: 3.1811 - val_actual_acc: 0.2180\n",
      "Epoch 138/500\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 1.3058 - actual_acc: 0.6044 - val_loss: 3.4095 - val_actual_acc: 0.2020\n",
      "Epoch 139/500\n",
      "100/100 [==============================] - 31s 310ms/step - loss: 1.4391 - actual_acc: 0.5844 - val_loss: 4.2928 - val_actual_acc: 0.1533\n",
      "Epoch 140/500\n",
      "100/100 [==============================] - 20s 196ms/step - loss: 1.7766 - actual_acc: 0.4813 - val_loss: 4.1063 - val_actual_acc: 0.1650\n",
      "Epoch 141/500\n",
      "100/100 [==============================] - 19s 186ms/step - loss: 1.4253 - actual_acc: 0.5806 - val_loss: 3.5370 - val_actual_acc: 0.1890\n",
      "Epoch 142/500\n",
      "100/100 [==============================] - 25s 248ms/step - loss: 1.1133 - actual_acc: 0.6688 - val_loss: 3.6374 - val_actual_acc: 0.1977\n",
      "Epoch 143/500\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 1.3432 - actual_acc: 0.6231 - val_loss: 3.4846 - val_actual_acc: 0.2035\n",
      "Epoch 144/500\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 2.0418 - actual_acc: 0.4044 - val_loss: 3.0943 - val_actual_acc: 0.2355\n",
      "Epoch 145/500\n",
      "100/100 [==============================] - 17s 173ms/step - loss: 1.4628 - actual_acc: 0.5513 - val_loss: 4.3777 - val_actual_acc: 0.1948\n",
      "Epoch 146/500\n",
      "100/100 [==============================] - 25s 246ms/step - loss: 1.3272 - actual_acc: 0.6038 - val_loss: 3.9088 - val_actual_acc: 0.1846\n",
      "Epoch 147/500\n",
      "100/100 [==============================] - 20s 201ms/step - loss: 1.2429 - actual_acc: 0.6369 - val_loss: 3.7605 - val_actual_acc: 0.1424\n",
      "Epoch 148/500\n",
      "100/100 [==============================] - 31s 310ms/step - loss: 1.6262 - actual_acc: 0.5325 - val_loss: 3.7016 - val_actual_acc: 0.1628\n",
      "Epoch 149/500\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 2.0903 - actual_acc: 0.4075 - val_loss: 3.1201 - val_actual_acc: 0.2006\n",
      "Epoch 150/500\n",
      "100/100 [==============================] - 21s 206ms/step - loss: 1.6227 - actual_acc: 0.5256 - val_loss: 3.0903 - val_actual_acc: 0.2282\n",
      "Epoch 151/500\n",
      "100/100 [==============================] - 27s 274ms/step - loss: 1.3271 - actual_acc: 0.5944 - val_loss: 3.7048 - val_actual_acc: 0.1664\n",
      "Epoch 152/500\n",
      "100/100 [==============================] - 17s 175ms/step - loss: 1.1496 - actual_acc: 0.6631 - val_loss: 3.1276 - val_actual_acc: 0.1744\n",
      "Epoch 153/500\n",
      "100/100 [==============================] - 35s 355ms/step - loss: 1.9373 - actual_acc: 0.4419 - val_loss: 2.8038 - val_actual_acc: 0.2376\n",
      "Epoch 154/500\n",
      "100/100 [==============================] - 18s 175ms/step - loss: 1.6308 - actual_acc: 0.5075 - val_loss: 3.6408 - val_actual_acc: 0.2035\n",
      "Epoch 155/500\n",
      "100/100 [==============================] - 26s 256ms/step - loss: 1.3090 - actual_acc: 0.6162 - val_loss: 2.7915 - val_actual_acc: 0.2645\n",
      "Epoch 156/500\n",
      "100/100 [==============================] - 25s 252ms/step - loss: 1.3148 - actual_acc: 0.6350 - val_loss: 3.9116 - val_actual_acc: 0.1635\n",
      "Epoch 157/500\n",
      "100/100 [==============================] - 27s 272ms/step - loss: 2.0212 - actual_acc: 0.4206 - val_loss: 3.3171 - val_actual_acc: 0.1672\n",
      "Epoch 158/500\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 1.4497 - actual_acc: 0.5669 - val_loss: 3.0829 - val_actual_acc: 0.1664\n",
      "Epoch 159/500\n",
      "100/100 [==============================] - 31s 314ms/step - loss: 1.2199 - actual_acc: 0.6475 - val_loss: 2.7661 - val_actual_acc: 0.2115\n",
      "Epoch 160/500\n",
      "100/100 [==============================] - 25s 253ms/step - loss: 1.2081 - actual_acc: 0.6569 - val_loss: 2.9955 - val_actual_acc: 0.2253\n",
      "Epoch 161/500\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 2.0889 - actual_acc: 0.4050 - val_loss: 2.7140 - val_actual_acc: 0.2565\n",
      "Epoch 162/500\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 1.6187 - actual_acc: 0.5250 - val_loss: 3.2053 - val_actual_acc: 0.1904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/500\n",
      "100/100 [==============================] - 26s 265ms/step - loss: 1.2680 - actual_acc: 0.6144 - val_loss: 2.7898 - val_actual_acc: 0.2435\n",
      "Epoch 164/500\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 1.5749 - actual_acc: 0.5625 - val_loss: 2.7287 - val_actual_acc: 0.2427\n",
      "Epoch 165/500\n",
      "100/100 [==============================] - 24s 242ms/step - loss: 1.7137 - actual_acc: 0.5019 - val_loss: 3.1112 - val_actual_acc: 0.2660\n",
      "Epoch 166/500\n",
      "100/100 [==============================] - 18s 177ms/step - loss: 1.3598 - actual_acc: 0.6075 - val_loss: 2.3245 - val_actual_acc: 0.2747\n",
      "Epoch 167/500\n",
      "100/100 [==============================] - 26s 262ms/step - loss: 1.1666 - actual_acc: 0.6575 - val_loss: 2.9039 - val_actual_acc: 0.2471\n",
      "Epoch 168/500\n",
      "100/100 [==============================] - 23s 226ms/step - loss: 1.4404 - actual_acc: 0.5844 - val_loss: 4.3699 - val_actual_acc: 0.1592\n",
      "Epoch 169/500\n",
      "100/100 [==============================] - 24s 244ms/step - loss: 1.8454 - actual_acc: 0.4375 - val_loss: 3.5012 - val_actual_acc: 0.1512\n",
      "Epoch 170/500\n",
      "100/100 [==============================] - 17s 172ms/step - loss: 1.2927 - actual_acc: 0.6000 - val_loss: 2.8867 - val_actual_acc: 0.2086\n",
      "Epoch 171/500\n",
      "100/100 [==============================] - 32s 319ms/step - loss: 1.5151 - actual_acc: 0.5606 - val_loss: 4.1731 - val_actual_acc: 0.1621\n",
      "Epoch 172/500\n",
      "100/100 [==============================] - 18s 177ms/step - loss: 1.8870 - actual_acc: 0.4381 - val_loss: 3.7160 - val_actual_acc: 0.2326\n",
      "Epoch 173/500\n",
      "100/100 [==============================] - 26s 264ms/step - loss: 1.4608 - actual_acc: 0.5544 - val_loss: 3.7449 - val_actual_acc: 0.2108\n",
      "Epoch 174/500\n",
      "100/100 [==============================] - 18s 177ms/step - loss: 1.2462 - actual_acc: 0.6156 - val_loss: 3.5047 - val_actual_acc: 0.2340\n",
      "Epoch 175/500\n",
      "100/100 [==============================] - 33s 335ms/step - loss: 1.6636 - actual_acc: 0.5288 - val_loss: 2.9712 - val_actual_acc: 0.2282\n",
      "Epoch 176/500\n",
      "100/100 [==============================] - 18s 178ms/step - loss: 1.6268 - actual_acc: 0.4937 - val_loss: 2.4397 - val_actual_acc: 0.2609\n",
      "Epoch 177/500\n",
      "100/100 [==============================] - 24s 242ms/step - loss: 1.3176 - actual_acc: 0.6150 - val_loss: 2.8291 - val_actual_acc: 0.2238\n",
      "Epoch 178/500\n",
      "100/100 [==============================] - 24s 241ms/step - loss: 1.1853 - actual_acc: 0.6219 - val_loss: 2.6872 - val_actual_acc: 0.2398\n",
      "Epoch 179/500\n",
      "100/100 [==============================] - 23s 235ms/step - loss: 1.6312 - actual_acc: 0.5281 - val_loss: 3.3087 - val_actual_acc: 0.2224\n",
      "Epoch 180/500\n",
      "100/100 [==============================] - 25s 255ms/step - loss: 1.5311 - actual_acc: 0.5363 - val_loss: 3.7809 - val_actual_acc: 0.1955\n",
      "Epoch 181/500\n",
      "100/100 [==============================] - 17s 172ms/step - loss: 1.1484 - actual_acc: 0.6562 - val_loss: 2.9518 - val_actual_acc: 0.1977\n",
      "Epoch 182/500\n",
      "100/100 [==============================] - 34s 338ms/step - loss: 1.7552 - actual_acc: 0.4944 - val_loss: 3.2206 - val_actual_acc: 0.1715\n",
      "Epoch 183/500\n",
      "100/100 [==============================] - 18s 175ms/step - loss: 1.6425 - actual_acc: 0.5062 - val_loss: 2.2742 - val_actual_acc: 0.2936\n",
      "Epoch 184/500\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 1.3001 - actual_acc: 0.6031 - val_loss: 2.6342 - val_actual_acc: 0.2624\n",
      "Epoch 185/500\n",
      "100/100 [==============================] - 18s 179ms/step - loss: 1.1180 - actual_acc: 0.6631 - val_loss: 2.6422 - val_actual_acc: 0.2282\n",
      "Epoch 186/500\n",
      "100/100 [==============================] - 34s 342ms/step - loss: 2.0554 - actual_acc: 0.4288 - val_loss: 2.7404 - val_actual_acc: 0.2398\n",
      "Epoch 187/500\n",
      "100/100 [==============================] - 18s 177ms/step - loss: 1.7293 - actual_acc: 0.4944 - val_loss: 2.8754 - val_actual_acc: 0.2391\n",
      "Epoch 188/500\n",
      "100/100 [==============================] - 26s 256ms/step - loss: 1.4368 - actual_acc: 0.5662 - val_loss: 2.6636 - val_actual_acc: 0.2456\n",
      "Epoch 189/500\n",
      "100/100 [==============================] - 17s 173ms/step - loss: 1.3206 - actual_acc: 0.5938 - val_loss: 3.1506 - val_actual_acc: 0.2151\n",
      "Epoch 190/500\n",
      "100/100 [==============================] - 34s 344ms/step - loss: 1.8679 - actual_acc: 0.4656 - val_loss: 2.8345 - val_actual_acc: 0.2064\n",
      "Epoch 191/500\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 1.3851 - actual_acc: 0.5869 - val_loss: 2.3542 - val_actual_acc: 0.2544\n",
      "Epoch 192/500\n",
      "100/100 [==============================] - 26s 256ms/step - loss: 1.0721 - actual_acc: 0.6700 - val_loss: 2.3362 - val_actual_acc: 0.2674\n",
      "Epoch 193/500\n",
      "100/100 [==============================] - 18s 179ms/step - loss: 1.0154 - actual_acc: 0.6994 - val_loss: 2.3967 - val_actual_acc: 0.2587\n",
      "Epoch 194/500\n",
      "100/100 [==============================] - 34s 345ms/step - loss: 1.8833 - actual_acc: 0.4669 - val_loss: 3.2091 - val_actual_acc: 0.1875\n",
      "Epoch 195/500\n",
      "100/100 [==============================] - 18s 177ms/step - loss: 1.5511 - actual_acc: 0.5463 - val_loss: 3.0674 - val_actual_acc: 0.2057\n",
      "Epoch 196/500\n",
      "100/100 [==============================] - 24s 241ms/step - loss: 1.2367 - actual_acc: 0.6338 - val_loss: 3.2723 - val_actual_acc: 0.2115\n",
      "Epoch 197/500\n",
      "100/100 [==============================] - 17s 173ms/step - loss: 1.1011 - actual_acc: 0.6737 - val_loss: 2.5995 - val_actual_acc: 0.2297\n",
      "Epoch 198/500\n",
      "100/100 [==============================] - 35s 346ms/step - loss: 1.4927 - actual_acc: 0.5787 - val_loss: 3.3745 - val_actual_acc: 0.2144\n",
      "Epoch 199/500\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 2.0147 - actual_acc: 0.4025 - val_loss: 3.5398 - val_actual_acc: 0.1541\n",
      "Epoch 200/500\n",
      "100/100 [==============================] - 25s 251ms/step - loss: 1.5603 - actual_acc: 0.5463 - val_loss: 2.2970 - val_actual_acc: 0.2892\n",
      "Epoch 201/500\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 1.2941 - actual_acc: 0.6019 - val_loss: 2.5629 - val_actual_acc: 0.2391\n",
      "Epoch 202/500\n",
      "100/100 [==============================] - 34s 340ms/step - loss: 1.7432 - actual_acc: 0.5119 - val_loss: 3.5803 - val_actual_acc: 0.2122\n",
      "Epoch 203/500\n",
      "100/100 [==============================] - 17s 173ms/step - loss: 1.7292 - actual_acc: 0.4931 - val_loss: 3.1644 - val_actual_acc: 0.1737\n",
      "Epoch 204/500\n",
      "100/100 [==============================] - 25s 252ms/step - loss: 1.4187 - actual_acc: 0.5750 - val_loss: 2.5696 - val_actual_acc: 0.2435\n",
      "Epoch 205/500\n",
      "100/100 [==============================] - 18s 178ms/step - loss: 1.1815 - actual_acc: 0.6350 - val_loss: 2.8233 - val_actual_acc: 0.1969\n",
      "Epoch 206/500\n",
      "100/100 [==============================] - 34s 343ms/step - loss: 1.6841 - actual_acc: 0.5206 - val_loss: 3.0210 - val_actual_acc: 0.2340\n",
      "Epoch 207/500\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 1.5876 - actual_acc: 0.5525 - val_loss: 2.3814 - val_actual_acc: 0.3096\n",
      "Epoch 208/500\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 1.2660 - actual_acc: 0.6475 - val_loss: 2.8160 - val_actual_acc: 0.2275\n",
      "Epoch 209/500\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 1.0741 - actual_acc: 0.6800 - val_loss: 2.4105 - val_actual_acc: 0.2754\n",
      "Epoch 210/500\n",
      "100/100 [==============================] - 34s 341ms/step - loss: 2.1068 - actual_acc: 0.4350 - val_loss: 2.7362 - val_actual_acc: 0.2129\n",
      "Epoch 211/500\n",
      "100/100 [==============================] - 17s 175ms/step - loss: 1.8862 - actual_acc: 0.4562 - val_loss: 2.2772 - val_actual_acc: 0.2398\n",
      "Epoch 212/500\n",
      "100/100 [==============================] - 23s 227ms/step - loss: 1.5479 - actual_acc: 0.5550 - val_loss: 2.3683 - val_actual_acc: 0.2493\n",
      "Epoch 213/500\n",
      "100/100 [==============================] - 24s 241ms/step - loss: 1.3461 - actual_acc: 0.6056 - val_loss: 2.5598 - val_actual_acc: 0.2195\n",
      "Epoch 214/500\n",
      "100/100 [==============================] - 25s 246ms/step - loss: 1.6902 - actual_acc: 0.4950 - val_loss: 2.7300 - val_actual_acc: 0.2834\n",
      "Epoch 215/500\n",
      "100/100 [==============================] - 26s 256ms/step - loss: 1.4911 - actual_acc: 0.5669 - val_loss: 2.5942 - val_actual_acc: 0.3198\n",
      "Epoch 216/500\n",
      "100/100 [==============================] - 18s 175ms/step - loss: 1.1163 - actual_acc: 0.6550 - val_loss: 2.4355 - val_actual_acc: 0.3045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 217/500\n",
      "100/100 [==============================] - 33s 328ms/step - loss: 0.9252 - actual_acc: 0.7244 - val_loss: 2.1960 - val_actual_acc: 0.3452\n",
      "Epoch 218/500\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 2.1440 - actual_acc: 0.3912 - val_loss: 2.5343 - val_actual_acc: 0.2551\n",
      "Epoch 219/500\n",
      "100/100 [==============================] - 24s 240ms/step - loss: 1.6104 - actual_acc: 0.5125 - val_loss: 2.5060 - val_actual_acc: 0.2580\n",
      "Epoch 220/500\n",
      "100/100 [==============================] - 17s 175ms/step - loss: 1.3680 - actual_acc: 0.5794 - val_loss: 2.4417 - val_actual_acc: 0.2478\n",
      "Epoch 221/500\n",
      "100/100 [==============================] - 69s 695ms/step - loss: 1.1161 - actual_acc: 0.6613 - val_loss: 2.6266 - val_actual_acc: 0.2260\n",
      "Epoch 222/500\n",
      "100/100 [==============================] - 25s 246ms/step - loss: 2.0069 - actual_acc: 0.4281 - val_loss: 2.8116 - val_actual_acc: 0.2435\n",
      "Epoch 223/500\n",
      "100/100 [==============================] - 17s 173ms/step - loss: 1.4622 - actual_acc: 0.5706 - val_loss: 2.7074 - val_actual_acc: 0.2718\n",
      "Epoch 224/500\n",
      "100/100 [==============================] - 23s 231ms/step - loss: 1.2145 - actual_acc: 0.6325 - val_loss: 2.5225 - val_actual_acc: 0.2791\n",
      "Epoch 225/500\n",
      "100/100 [==============================] - 26s 261ms/step - loss: 1.2096 - actual_acc: 0.6400 - val_loss: 2.3811 - val_actual_acc: 0.2602\n",
      "Epoch 226/500\n",
      "100/100 [==============================] - 27s 265ms/step - loss: 1.9223 - actual_acc: 0.4494 - val_loss: 2.8317 - val_actual_acc: 0.2267\n",
      "Epoch 227/500\n",
      "100/100 [==============================] - 17s 173ms/step - loss: 1.5351 - actual_acc: 0.5288 - val_loss: 2.8455 - val_actual_acc: 0.2318\n",
      "Epoch 228/500\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 1.2614 - actual_acc: 0.6038 - val_loss: 2.9902 - val_actual_acc: 0.2057\n",
      "Epoch 229/500\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 1.3999 - actual_acc: 0.5906 - val_loss: 3.3517 - val_actual_acc: 0.1839\n",
      "Epoch 230/500\n",
      "100/100 [==============================] - 25s 255ms/step - loss: 2.0048 - actual_acc: 0.4144 - val_loss: 3.1934 - val_actual_acc: 0.1860\n",
      "Epoch 231/500\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 1.5679 - actual_acc: 0.5381 - val_loss: 2.7829 - val_actual_acc: 0.1999\n",
      "Epoch 232/500\n",
      "100/100 [==============================] - 26s 255ms/step - loss: 1.3313 - actual_acc: 0.5769 - val_loss: 2.4820 - val_actual_acc: 0.2340\n",
      "Epoch 233/500\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 1.3811 - actual_acc: 0.5894 - val_loss: 2.7486 - val_actual_acc: 0.2020\n",
      "Epoch 234/500\n",
      "100/100 [==============================] - 26s 256ms/step - loss: 1.8694 - actual_acc: 0.4438 - val_loss: 3.2415 - val_actual_acc: 0.1955\n",
      "Epoch 235/500\n",
      "100/100 [==============================] - 18s 177ms/step - loss: 1.4796 - actual_acc: 0.5431 - val_loss: 3.0896 - val_actual_acc: 0.1512\n",
      "Epoch 236/500\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 1.2641 - actual_acc: 0.6162 - val_loss: 2.5771 - val_actual_acc: 0.2376\n",
      "Epoch 237/500\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 1.4798 - actual_acc: 0.5525 - val_loss: 2.0908 - val_actual_acc: 0.3263\n",
      "Epoch 238/500\n",
      "100/100 [==============================] - 26s 261ms/step - loss: 1.9705 - actual_acc: 0.4225 - val_loss: 2.2220 - val_actual_acc: 0.2863\n",
      "Epoch 239/500\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 1.6240 - actual_acc: 0.5262 - val_loss: 2.5600 - val_actual_acc: 0.2006\n",
      "Epoch 240/500\n",
      "100/100 [==============================] - 25s 248ms/step - loss: 1.3750 - actual_acc: 0.5887 - val_loss: 2.3798 - val_actual_acc: 0.2943\n",
      "Epoch 241/500\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 1.6547 - actual_acc: 0.5100 - val_loss: 2.4906 - val_actual_acc: 0.2733\n",
      "Epoch 242/500\n",
      "100/100 [==============================] - 24s 244ms/step - loss: 1.6406 - actual_acc: 0.5131 - val_loss: 2.7638 - val_actual_acc: 0.2674\n",
      "Epoch 243/500\n",
      "100/100 [==============================] - 17s 172ms/step - loss: 1.3926 - actual_acc: 0.5744 - val_loss: 2.4808 - val_actual_acc: 0.2645\n",
      "Epoch 244/500\n",
      "100/100 [==============================] - 25s 246ms/step - loss: 1.1373 - actual_acc: 0.6412 - val_loss: 2.5572 - val_actual_acc: 0.2318\n",
      "Epoch 245/500\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 1.7863 - actual_acc: 0.4700 - val_loss: 2.4290 - val_actual_acc: 0.2776\n",
      "Epoch 246/500\n",
      "100/100 [==============================] - 26s 262ms/step - loss: 1.5259 - actual_acc: 0.5369 - val_loss: 2.7518 - val_actual_acc: 0.2435\n",
      "Epoch 247/500\n",
      "100/100 [==============================] - 17s 172ms/step - loss: 1.3298 - actual_acc: 0.6000 - val_loss: 2.7053 - val_actual_acc: 0.2151\n",
      "Epoch 248/500\n",
      "100/100 [==============================] - 25s 251ms/step - loss: 1.1140 - actual_acc: 0.6637 - val_loss: 2.4550 - val_actual_acc: 0.2674\n",
      "Epoch 249/500\n",
      "100/100 [==============================] - 28s 276ms/step - loss: 2.0650 - actual_acc: 0.4150 - val_loss: 2.2897 - val_actual_acc: 0.2863\n",
      "Epoch 250/500\n",
      "100/100 [==============================] - 20s 198ms/step - loss: 1.5931 - actual_acc: 0.5256 - val_loss: 2.3299 - val_actual_acc: 0.2624\n",
      "Epoch 251/500\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 1.3356 - actual_acc: 0.5969 - val_loss: 2.6003 - val_actual_acc: 0.2355\n",
      "Epoch 252/500\n",
      "100/100 [==============================] - 18s 178ms/step - loss: 1.1500 - actual_acc: 0.6412 - val_loss: 2.6383 - val_actual_acc: 0.2326\n",
      "Epoch 253/500\n",
      "100/100 [==============================] - 34s 344ms/step - loss: 1.8149 - actual_acc: 0.4831 - val_loss: 2.6837 - val_actual_acc: 0.2180\n",
      "Epoch 254/500\n",
      "100/100 [==============================] - 17s 173ms/step - loss: 1.4621 - actual_acc: 0.5669 - val_loss: 2.4968 - val_actual_acc: 0.2638\n",
      "Epoch 255/500\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 1.2302 - actual_acc: 0.6175 - val_loss: 2.5504 - val_actual_acc: 0.2682\n",
      "Epoch 256/500\n",
      "100/100 [==============================] - 21s 211ms/step - loss: 1.0425 - actual_acc: 0.6844 - val_loss: 2.3151 - val_actual_acc: 0.3321\n",
      "Epoch 257/500\n",
      "100/100 [==============================] - 30s 299ms/step - loss: 1.9206 - actual_acc: 0.4562 - val_loss: 2.4159 - val_actual_acc: 0.3212\n",
      "Epoch 258/500\n",
      "100/100 [==============================] - 17s 173ms/step - loss: 1.4754 - actual_acc: 0.5669 - val_loss: 2.1447 - val_actual_acc: 0.3278\n",
      "Epoch 259/500\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 1.1666 - actual_acc: 0.6481 - val_loss: 2.2265 - val_actual_acc: 0.2856\n",
      "Epoch 260/500\n",
      "100/100 [==============================] - 19s 188ms/step - loss: 1.0186 - actual_acc: 0.6875 - val_loss: 2.4535 - val_actual_acc: 0.2209\n",
      "Epoch 261/500\n",
      "100/100 [==============================] - 28s 282ms/step - loss: 1.9675 - actual_acc: 0.4331 - val_loss: 2.9195 - val_actual_acc: 0.2180\n",
      "Epoch 262/500\n",
      "100/100 [==============================] - 17s 172ms/step - loss: 1.3598 - actual_acc: 0.5869 - val_loss: 3.0345 - val_actual_acc: 0.1933\n",
      "Epoch 263/500\n",
      "100/100 [==============================] - 24s 242ms/step - loss: 1.0618 - actual_acc: 0.6688 - val_loss: 2.4937 - val_actual_acc: 0.2529\n",
      "Epoch 264/500\n",
      "100/100 [==============================] - 23s 228ms/step - loss: 1.8748 - actual_acc: 0.4644 - val_loss: 2.7382 - val_actual_acc: 0.2376\n",
      "Epoch 265/500\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 1.4708 - actual_acc: 0.5463 - val_loss: 2.7932 - val_actual_acc: 0.2129\n",
      "Epoch 266/500\n",
      "100/100 [==============================] - 17s 173ms/step - loss: 1.1889 - actual_acc: 0.6425 - val_loss: 2.5998 - val_actual_acc: 0.2042\n",
      "Epoch 267/500\n",
      "100/100 [==============================] - 32s 316ms/step - loss: 1.6683 - actual_acc: 0.5294 - val_loss: 3.3206 - val_actual_acc: 0.1977\n",
      "Epoch 268/500\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 1.8497 - actual_acc: 0.4606 - val_loss: 3.5673 - val_actual_acc: 0.2013\n",
      "Epoch 269/500\n",
      "100/100 [==============================] - 24s 242ms/step - loss: 1.4469 - actual_acc: 0.5694 - val_loss: 3.0385 - val_actual_acc: 0.2020\n",
      "Epoch 270/500\n",
      "100/100 [==============================] - 18s 178ms/step - loss: 1.2899 - actual_acc: 0.6050 - val_loss: 2.4651 - val_actual_acc: 0.2311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 271/500\n",
      "100/100 [==============================] - 36s 357ms/step - loss: 1.8128 - actual_acc: 0.4963 - val_loss: 2.6045 - val_actual_acc: 0.2515\n",
      "Epoch 272/500\n",
      "100/100 [==============================] - 18s 178ms/step - loss: 1.5921 - actual_acc: 0.5281 - val_loss: 2.7894 - val_actual_acc: 0.2224\n",
      "Epoch 273/500\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 1.3333 - actual_acc: 0.6012 - val_loss: 2.2677 - val_actual_acc: 0.2834\n",
      "Epoch 274/500\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 1.1251 - actual_acc: 0.6681 - val_loss: 2.1441 - val_actual_acc: 0.3227\n",
      "Epoch 275/500\n",
      "100/100 [==============================] - 30s 304ms/step - loss: 1.7084 - actual_acc: 0.5238 - val_loss: 2.8776 - val_actual_acc: 0.2071\n",
      "Epoch 276/500\n",
      "100/100 [==============================] - 17s 173ms/step - loss: 1.7872 - actual_acc: 0.4706 - val_loss: 2.1553 - val_actual_acc: 0.3263\n",
      "Epoch 277/500\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 1.3602 - actual_acc: 0.5975 - val_loss: 2.2521 - val_actual_acc: 0.2791\n",
      "Epoch 278/500\n",
      "100/100 [==============================] - 18s 179ms/step - loss: 1.1456 - actual_acc: 0.6600 - val_loss: 2.3508 - val_actual_acc: 0.2660\n",
      "Epoch 279/500\n",
      "100/100 [==============================] - 36s 360ms/step - loss: 1.7501 - actual_acc: 0.5081 - val_loss: 2.2711 - val_actual_acc: 0.2769\n",
      "Epoch 280/500\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 1.6047 - actual_acc: 0.5231 - val_loss: 2.0302 - val_actual_acc: 0.3278\n",
      "Epoch 281/500\n",
      "100/100 [==============================] - 21s 208ms/step - loss: 1.2890 - actual_acc: 0.6094 - val_loss: 2.2402 - val_actual_acc: 0.2929\n",
      "Epoch 282/500\n",
      "100/100 [==============================] - 18s 179ms/step - loss: 1.1561 - actual_acc: 0.6450 - val_loss: 2.4106 - val_actual_acc: 0.2711\n",
      "Epoch 283/500\n",
      "100/100 [==============================] - 56s 562ms/step - loss: 1.8163 - actual_acc: 0.4738 - val_loss: 3.0809 - val_actual_acc: 0.2137\n",
      "Epoch 284/500\n",
      "100/100 [==============================] - 18s 180ms/step - loss: 1.5658 - actual_acc: 0.5275 - val_loss: 3.0490 - val_actual_acc: 0.2028\n",
      "Epoch 285/500\n",
      "100/100 [==============================] - 20s 205ms/step - loss: 1.2784 - actual_acc: 0.6112 - val_loss: 2.2084 - val_actual_acc: 0.2914\n",
      "Epoch 286/500\n",
      "100/100 [==============================] - 30s 298ms/step - loss: 1.0928 - actual_acc: 0.6512 - val_loss: 2.2621 - val_actual_acc: 0.3045\n",
      "Epoch 287/500\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 1.9132 - actual_acc: 0.4438 - val_loss: 2.3845 - val_actual_acc: 0.2885\n",
      "Epoch 288/500\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 1.4299 - actual_acc: 0.5775 - val_loss: 2.2884 - val_actual_acc: 0.3089\n",
      "Epoch 289/500\n",
      "100/100 [==============================] - 18s 179ms/step - loss: 1.1200 - actual_acc: 0.6681 - val_loss: 2.5408 - val_actual_acc: 0.2754\n",
      "Epoch 290/500\n",
      "100/100 [==============================] - 46s 458ms/step - loss: 1.0018 - actual_acc: 0.7013 - val_loss: 2.3370 - val_actual_acc: 0.3183\n",
      "Epoch 291/500\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 2.0230 - actual_acc: 0.4237 - val_loss: 2.5765 - val_actual_acc: 0.2406\n",
      "Epoch 292/500\n",
      "100/100 [==============================] - 22s 224ms/step - loss: 1.4858 - actual_acc: 0.5631 - val_loss: 2.5804 - val_actual_acc: 0.2849\n",
      "Epoch 293/500\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 1.2715 - actual_acc: 0.6244 - val_loss: 2.6154 - val_actual_acc: 0.2369\n",
      "Epoch 294/500\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 1.0814 - actual_acc: 0.6762 - val_loss: 2.6004 - val_actual_acc: 0.2297\n",
      "Epoch 295/500\n",
      "100/100 [==============================] - 23s 227ms/step - loss: 1.6034 - actual_acc: 0.5238 - val_loss: 2.4633 - val_actual_acc: 0.2660\n",
      "Epoch 296/500\n",
      "100/100 [==============================] - 23s 226ms/step - loss: 1.9010 - actual_acc: 0.4531 - val_loss: 2.2669 - val_actual_acc: 0.3140\n",
      "Epoch 297/500\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 1.5592 - actual_acc: 0.5419 - val_loss: 1.9816 - val_actual_acc: 0.3270\n",
      "Epoch 298/500\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 1.2231 - actual_acc: 0.6300 - val_loss: 2.2091 - val_actual_acc: 0.2689\n",
      "Epoch 299/500\n",
      "100/100 [==============================] - 26s 265ms/step - loss: 0.9817 - actual_acc: 0.6931 - val_loss: 2.4247 - val_actual_acc: 0.2776\n",
      "Epoch 300/500\n",
      "100/100 [==============================] - 25s 252ms/step - loss: 1.7666 - actual_acc: 0.4931 - val_loss: 2.1865 - val_actual_acc: 0.3161\n",
      "Epoch 301/500\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 1.5672 - actual_acc: 0.5200 - val_loss: 2.2142 - val_actual_acc: 0.2922\n",
      "Epoch 302/500\n",
      "100/100 [==============================] - 17s 173ms/step - loss: 1.2179 - actual_acc: 0.6369 - val_loss: 2.5430 - val_actual_acc: 0.2754\n",
      "Epoch 303/500\n",
      "100/100 [==============================] - 31s 315ms/step - loss: 1.0721 - actual_acc: 0.6656 - val_loss: 2.5948 - val_actual_acc: 0.2311\n",
      "Epoch 304/500\n",
      "100/100 [==============================] - 18s 175ms/step - loss: 1.9874 - actual_acc: 0.4431 - val_loss: 2.7057 - val_actual_acc: 0.2507\n",
      "Epoch 305/500\n",
      "100/100 [==============================] - 25s 253ms/step - loss: 1.4787 - actual_acc: 0.5825 - val_loss: 2.6178 - val_actual_acc: 0.2507\n",
      "Epoch 306/500\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 1.2102 - actual_acc: 0.6525 - val_loss: 2.6925 - val_actual_acc: 0.2413\n",
      "Epoch 307/500\n",
      "100/100 [==============================] - 55s 547ms/step - loss: 1.0374 - actual_acc: 0.6894 - val_loss: 2.4397 - val_actual_acc: 0.2536\n",
      "Epoch 308/500\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 2.0313 - actual_acc: 0.4144 - val_loss: 2.3912 - val_actual_acc: 0.2624\n",
      "Epoch 309/500\n",
      "100/100 [==============================] - 25s 254ms/step - loss: 1.5110 - actual_acc: 0.5487 - val_loss: 2.3232 - val_actual_acc: 0.2827\n",
      "Epoch 310/500\n",
      "100/100 [==============================] - 17s 173ms/step - loss: 1.2473 - actual_acc: 0.6419 - val_loss: 2.3299 - val_actual_acc: 0.2922\n",
      "Epoch 311/500\n",
      "100/100 [==============================] - 33s 326ms/step - loss: 1.2890 - actual_acc: 0.6169 - val_loss: 2.1815 - val_actual_acc: 0.2805\n",
      "Epoch 312/500\n",
      "100/100 [==============================] - 18s 175ms/step - loss: 1.8316 - actual_acc: 0.4694 - val_loss: 2.1901 - val_actual_acc: 0.3169\n",
      "Epoch 313/500\n",
      "100/100 [==============================] - 26s 260ms/step - loss: 1.3175 - actual_acc: 0.6056 - val_loss: 2.3592 - val_actual_acc: 0.2682\n",
      "Epoch 314/500\n",
      "100/100 [==============================] - 18s 178ms/step - loss: 1.0848 - actual_acc: 0.6606 - val_loss: 2.5188 - val_actual_acc: 0.2500\n",
      "Epoch 315/500\n",
      "100/100 [==============================] - 30s 299ms/step - loss: 1.1134 - actual_acc: 0.6644 - val_loss: 2.7286 - val_actual_acc: 0.2137\n",
      "Epoch 316/500\n",
      "100/100 [==============================] - 17s 172ms/step - loss: 1.8591 - actual_acc: 0.4619 - val_loss: 2.3566 - val_actual_acc: 0.2791\n",
      "Epoch 317/500\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 1.4192 - actual_acc: 0.5800 - val_loss: 2.3465 - val_actual_acc: 0.2544\n",
      "Epoch 318/500\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 1.2692 - actual_acc: 0.6125 - val_loss: 2.3958 - val_actual_acc: 0.2638\n",
      "Epoch 319/500\n",
      "100/100 [==============================] - 25s 254ms/step - loss: 1.8179 - actual_acc: 0.4675 - val_loss: 2.5056 - val_actual_acc: 0.2740\n",
      "Epoch 320/500\n",
      "100/100 [==============================] - 17s 173ms/step - loss: 1.4326 - actual_acc: 0.5656 - val_loss: 2.4957 - val_actual_acc: 0.2500\n",
      "Epoch 321/500\n",
      "100/100 [==============================] - 22s 219ms/step - loss: 1.2419 - actual_acc: 0.6206 - val_loss: 2.4471 - val_actual_acc: 0.2347\n",
      "Epoch 322/500\n",
      "100/100 [==============================] - 27s 265ms/step - loss: 1.4483 - actual_acc: 0.5819 - val_loss: 2.4166 - val_actual_acc: 0.3001\n",
      "Epoch 323/500\n",
      "100/100 [==============================] - 20s 202ms/step - loss: 1.6850 - actual_acc: 0.4981 - val_loss: 2.2634 - val_actual_acc: 0.3270\n",
      "Epoch 324/500\n",
      "100/100 [==============================] - 24s 244ms/step - loss: 1.3456 - actual_acc: 0.5975 - val_loss: 2.4024 - val_actual_acc: 0.2725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 325/500\n",
      "100/100 [==============================] - 18s 179ms/step - loss: 1.0738 - actual_acc: 0.6656 - val_loss: 2.5566 - val_actual_acc: 0.2297\n",
      "Epoch 326/500\n",
      "100/100 [==============================] - 32s 322ms/step - loss: 1.3814 - actual_acc: 0.6019 - val_loss: 2.5262 - val_actual_acc: 0.2638\n",
      "Epoch 327/500\n",
      "100/100 [==============================] - 17s 173ms/step - loss: 1.5112 - actual_acc: 0.5550 - val_loss: 2.3630 - val_actual_acc: 0.2522\n",
      "Epoch 328/500\n",
      "100/100 [==============================] - 26s 263ms/step - loss: 1.0921 - actual_acc: 0.6688 - val_loss: 2.6149 - val_actual_acc: 0.2398\n",
      "Epoch 329/500\n",
      "100/100 [==============================] - 25s 247ms/step - loss: 1.4975 - actual_acc: 0.5631 - val_loss: 2.4017 - val_actual_acc: 0.2682\n",
      "Epoch 330/500\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 1.6517 - actual_acc: 0.4963 - val_loss: 2.4321 - val_actual_acc: 0.3067\n",
      "Epoch 331/500\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 1.3802 - actual_acc: 0.5581 - val_loss: 2.3118 - val_actual_acc: 0.2318\n",
      "Epoch 332/500\n",
      "100/100 [==============================] - 26s 264ms/step - loss: 1.0560 - actual_acc: 0.6775 - val_loss: 2.3682 - val_actual_acc: 0.2507\n",
      "Epoch 333/500\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 1.7883 - actual_acc: 0.5062 - val_loss: 2.1890 - val_actual_acc: 0.2958\n",
      "Epoch 334/500\n",
      "100/100 [==============================] - 26s 256ms/step - loss: 1.6903 - actual_acc: 0.4937 - val_loss: 2.1766 - val_actual_acc: 0.2892\n",
      "Epoch 335/500\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 1.4517 - actual_acc: 0.5550 - val_loss: 2.2050 - val_actual_acc: 0.2660\n",
      "Epoch 336/500\n",
      "100/100 [==============================] - 25s 247ms/step - loss: 1.2216 - actual_acc: 0.6225 - val_loss: 2.4152 - val_actual_acc: 0.2340\n",
      "Epoch 337/500\n",
      "100/100 [==============================] - 26s 263ms/step - loss: 1.7329 - actual_acc: 0.4888 - val_loss: 2.3514 - val_actual_acc: 0.2703\n",
      "Epoch 338/500\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 1.3157 - actual_acc: 0.6112 - val_loss: 2.3126 - val_actual_acc: 0.2856\n",
      "Epoch 339/500\n",
      "100/100 [==============================] - 17s 173ms/step - loss: 1.0470 - actual_acc: 0.6769 - val_loss: 2.3900 - val_actual_acc: 0.2682\n",
      "Epoch 340/500\n",
      "100/100 [==============================] - 24s 243ms/step - loss: 0.9294 - actual_acc: 0.7219 - val_loss: 2.4099 - val_actual_acc: 0.2638\n",
      "Epoch 341/500\n",
      "100/100 [==============================] - 28s 278ms/step - loss: 1.6183 - actual_acc: 0.5513 - val_loss: 2.4785 - val_actual_acc: 0.2500\n",
      "Epoch 342/500\n",
      "100/100 [==============================] - 24s 244ms/step - loss: 1.4847 - actual_acc: 0.5569 - val_loss: 2.2817 - val_actual_acc: 0.2863\n",
      "Epoch 343/500\n",
      "100/100 [==============================] - 17s 173ms/step - loss: 1.2449 - actual_acc: 0.6150 - val_loss: 2.3315 - val_actual_acc: 0.2878\n",
      "Epoch 344/500\n",
      "100/100 [==============================] - 21s 206ms/step - loss: 1.0094 - actual_acc: 0.6794 - val_loss: 2.5770 - val_actual_acc: 0.2624\n",
      "Epoch 345/500\n",
      "100/100 [==============================] - 23s 228ms/step - loss: 1.3213 - actual_acc: 0.6088 - val_loss: 2.7012 - val_actual_acc: 0.2536\n",
      "Epoch 346/500\n",
      "100/100 [==============================] - 27s 270ms/step - loss: 1.9709 - actual_acc: 0.4319 - val_loss: 2.5550 - val_actual_acc: 0.2485\n",
      "Epoch 347/500\n",
      "100/100 [==============================] - 17s 175ms/step - loss: 1.4804 - actual_acc: 0.5413 - val_loss: 2.1886 - val_actual_acc: 0.2907\n",
      "Epoch 348/500\n",
      "100/100 [==============================] - 25s 246ms/step - loss: 1.2535 - actual_acc: 0.6281 - val_loss: 2.6590 - val_actual_acc: 0.2355\n",
      "Epoch 349/500\n",
      "100/100 [==============================] - 27s 269ms/step - loss: 1.5907 - actual_acc: 0.5387 - val_loss: 2.6689 - val_actual_acc: 0.2122\n",
      "Epoch 350/500\n",
      "100/100 [==============================] - 25s 254ms/step - loss: 1.6481 - actual_acc: 0.5138 - val_loss: 1.9847 - val_actual_acc: 0.3314\n",
      "Epoch 351/500\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 1.3949 - actual_acc: 0.5831 - val_loss: 2.1498 - val_actual_acc: 0.2936\n",
      "Epoch 352/500\n",
      "100/100 [==============================] - 22s 220ms/step - loss: 1.1414 - actual_acc: 0.6706 - val_loss: 2.2168 - val_actual_acc: 0.2965\n",
      "Epoch 353/500\n",
      "100/100 [==============================] - 25s 253ms/step - loss: 1.5111 - actual_acc: 0.5669 - val_loss: 2.0431 - val_actual_acc: 0.3467\n",
      "Epoch 354/500\n",
      "100/100 [==============================] - 22s 215ms/step - loss: 1.5525 - actual_acc: 0.5575 - val_loss: 2.2013 - val_actual_acc: 0.3198\n",
      "Epoch 355/500\n",
      "100/100 [==============================] - 18s 177ms/step - loss: 1.2017 - actual_acc: 0.6344 - val_loss: 2.3597 - val_actual_acc: 0.3001\n",
      "Epoch 356/500\n",
      "100/100 [==============================] - 22s 218ms/step - loss: 1.0903 - actual_acc: 0.6756 - val_loss: 2.3002 - val_actual_acc: 0.3038\n",
      "Epoch 357/500\n",
      "100/100 [==============================] - 27s 268ms/step - loss: 1.9686 - actual_acc: 0.4800 - val_loss: 2.4035 - val_actual_acc: 0.2674\n",
      "Epoch 358/500\n",
      "100/100 [==============================] - 20s 199ms/step - loss: 1.8845 - actual_acc: 0.4356 - val_loss: 2.1334 - val_actual_acc: 0.3096\n",
      "Epoch 359/500\n",
      "100/100 [==============================] - 23s 228ms/step - loss: 1.5420 - actual_acc: 0.5444 - val_loss: 2.1991 - val_actual_acc: 0.2769\n",
      "Epoch 360/500\n",
      "100/100 [==============================] - 17s 173ms/step - loss: 1.3290 - actual_acc: 0.5956 - val_loss: 2.1948 - val_actual_acc: 0.3016\n",
      "Epoch 361/500\n",
      "100/100 [==============================] - 32s 315ms/step - loss: 1.6113 - actual_acc: 0.5256 - val_loss: 2.2952 - val_actual_acc: 0.2994\n",
      "Epoch 362/500\n",
      "100/100 [==============================] - 18s 177ms/step - loss: 1.4264 - actual_acc: 0.5656 - val_loss: 2.0244 - val_actual_acc: 0.3626\n",
      "Epoch 363/500\n",
      "100/100 [==============================] - 27s 269ms/step - loss: 1.1337 - actual_acc: 0.6544 - val_loss: 2.2404 - val_actual_acc: 0.3154\n",
      "Epoch 364/500\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 0.8567 - actual_acc: 0.7325 - val_loss: 2.5294 - val_actual_acc: 0.2616\n",
      "Epoch 365/500\n",
      "100/100 [==============================] - 33s 329ms/step - loss: 1.9063 - actual_acc: 0.4581 - val_loss: 2.7912 - val_actual_acc: 0.2500\n",
      "Epoch 366/500\n",
      "100/100 [==============================] - 18s 175ms/step - loss: 1.5863 - actual_acc: 0.5337 - val_loss: 2.3262 - val_actual_acc: 0.2914\n",
      "Epoch 367/500\n",
      "100/100 [==============================] - 21s 207ms/step - loss: 1.3414 - actual_acc: 0.5894 - val_loss: 2.1770 - val_actual_acc: 0.2900\n",
      "Epoch 368/500\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 1.1382 - actual_acc: 0.6363 - val_loss: 2.1536 - val_actual_acc: 0.2900\n",
      "Epoch 369/500\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 1.7738 - actual_acc: 0.4756 - val_loss: 2.3560 - val_actual_acc: 0.3023\n",
      "Epoch 370/500\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 1.4093 - actual_acc: 0.5887 - val_loss: 2.1733 - val_actual_acc: 0.3154\n",
      "Epoch 371/500\n",
      "100/100 [==============================] - 17s 173ms/step - loss: 1.1680 - actual_acc: 0.6412 - val_loss: 2.1771 - val_actual_acc: 0.2791\n",
      "Epoch 372/500\n",
      "100/100 [==============================] - 60s 597ms/step - loss: 0.9559 - actual_acc: 0.7119 - val_loss: 2.2128 - val_actual_acc: 0.2834\n",
      "Epoch 373/500\n",
      "100/100 [==============================] - 18s 178ms/step - loss: 1.9858 - actual_acc: 0.4456 - val_loss: 2.1574 - val_actual_acc: 0.2987\n",
      "Epoch 374/500\n",
      "100/100 [==============================] - 27s 272ms/step - loss: 1.5050 - actual_acc: 0.5487 - val_loss: 2.4092 - val_actual_acc: 0.2500\n",
      "Epoch 375/500\n",
      "100/100 [==============================] - 17s 173ms/step - loss: 1.2469 - actual_acc: 0.6106 - val_loss: 2.2360 - val_actual_acc: 0.2834\n",
      "Epoch 376/500\n",
      "100/100 [==============================] - 34s 341ms/step - loss: 1.1171 - actual_acc: 0.6719 - val_loss: 2.4873 - val_actual_acc: 0.2413\n",
      "Epoch 377/500\n",
      "100/100 [==============================] - 17s 175ms/step - loss: 2.0510 - actual_acc: 0.4138 - val_loss: 2.3908 - val_actual_acc: 0.2827\n",
      "Epoch 378/500\n",
      "100/100 [==============================] - 27s 265ms/step - loss: 1.4785 - actual_acc: 0.5675 - val_loss: 2.3701 - val_actual_acc: 0.2689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 379/500\n",
      "100/100 [==============================] - 18s 177ms/step - loss: 1.3057 - actual_acc: 0.6075 - val_loss: 2.4242 - val_actual_acc: 0.2842\n",
      "Epoch 380/500\n",
      "100/100 [==============================] - 35s 347ms/step - loss: 1.1239 - actual_acc: 0.6419 - val_loss: 2.1957 - val_actual_acc: 0.2994\n",
      "Epoch 381/500\n",
      "100/100 [==============================] - 18s 178ms/step - loss: 2.0027 - actual_acc: 0.4406 - val_loss: 2.6367 - val_actual_acc: 0.2195\n",
      "Epoch 382/500\n",
      "100/100 [==============================] - 28s 280ms/step - loss: 1.4813 - actual_acc: 0.5387 - val_loss: 2.0967 - val_actual_acc: 0.3299\n",
      "Epoch 383/500\n",
      "100/100 [==============================] - 18s 175ms/step - loss: 1.2792 - actual_acc: 0.6075 - val_loss: 1.9837 - val_actual_acc: 0.3358\n",
      "Epoch 384/500\n",
      "100/100 [==============================] - 32s 322ms/step - loss: 1.2814 - actual_acc: 0.6100 - val_loss: 1.9375 - val_actual_acc: 0.3394\n",
      "Epoch 385/500\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 1.9584 - actual_acc: 0.4331 - val_loss: 1.9506 - val_actual_acc: 0.3743\n",
      "Epoch 386/500\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 1.5669 - actual_acc: 0.5513 - val_loss: 2.1123 - val_actual_acc: 0.3031\n",
      "Epoch 387/500\n",
      "100/100 [==============================] - 18s 177ms/step - loss: 1.3460 - actual_acc: 0.5763 - val_loss: 2.2677 - val_actual_acc: 0.2769\n",
      "Epoch 388/500\n",
      "  3/100 [..............................] - ETA: 2:00 - loss: 1.1953 - actual_acc: 0.6875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Q:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (1.536896). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 32s 322ms/step - loss: 1.5068 - actual_acc: 0.5500 - val_loss: 2.1730 - val_actual_acc: 0.2849\n",
      "Epoch 389/500\n",
      "100/100 [==============================] - 17s 175ms/step - loss: 1.6816 - actual_acc: 0.4888 - val_loss: 2.0463 - val_actual_acc: 0.3009\n",
      "Epoch 390/500\n",
      "100/100 [==============================] - 23s 226ms/step - loss: 1.3507 - actual_acc: 0.5763 - val_loss: 2.2304 - val_actual_acc: 0.2892\n",
      "Epoch 391/500\n",
      "100/100 [==============================] - 18s 178ms/step - loss: 1.1527 - actual_acc: 0.6388 - val_loss: 2.2158 - val_actual_acc: 0.2645\n",
      "Epoch 392/500\n",
      "100/100 [==============================] - 30s 305ms/step - loss: 1.6456 - actual_acc: 0.5344 - val_loss: 2.2205 - val_actual_acc: 0.2951\n",
      "Epoch 393/500\n",
      "100/100 [==============================] - 18s 178ms/step - loss: 1.5757 - actual_acc: 0.5225 - val_loss: 2.0708 - val_actual_acc: 0.3001\n",
      "Epoch 394/500\n",
      "100/100 [==============================] - 21s 214ms/step - loss: 1.3810 - actual_acc: 0.5938 - val_loss: 1.9386 - val_actual_acc: 0.3241\n",
      "Epoch 395/500\n",
      "100/100 [==============================] - 18s 178ms/step - loss: 1.1127 - actual_acc: 0.6694 - val_loss: 1.9042 - val_actual_acc: 0.3539\n",
      "Epoch 396/500\n",
      "100/100 [==============================] - 29s 291ms/step - loss: 1.9876 - actual_acc: 0.4275 - val_loss: 1.9814 - val_actual_acc: 0.3408\n",
      "Epoch 397/500\n",
      "100/100 [==============================] - 24s 244ms/step - loss: 1.6057 - actual_acc: 0.5187 - val_loss: 2.1678 - val_actual_acc: 0.2885\n",
      "Epoch 398/500\n",
      "100/100 [==============================] - 18s 177ms/step - loss: 1.3696 - actual_acc: 0.5919 - val_loss: 2.1461 - val_actual_acc: 0.2529\n",
      "Epoch 399/500\n",
      "100/100 [==============================] - 26s 263ms/step - loss: 1.1429 - actual_acc: 0.6462 - val_loss: 2.2989 - val_actual_acc: 0.2718\n",
      "Epoch 400/500\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 1.5868 - actual_acc: 0.5387 - val_loss: 2.4401 - val_actual_acc: 0.2565\n",
      "Epoch 401/500\n",
      "100/100 [==============================] - 28s 278ms/step - loss: 1.4460 - actual_acc: 0.5819 - val_loss: 2.2373 - val_actual_acc: 0.2907\n",
      "Epoch 402/500\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 1.1982 - actual_acc: 0.6363 - val_loss: 2.1900 - val_actual_acc: 0.2980\n",
      "Epoch 403/500\n",
      "100/100 [==============================] - 26s 261ms/step - loss: 1.0523 - actual_acc: 0.6863 - val_loss: 1.9965 - val_actual_acc: 0.3299\n",
      "Epoch 404/500\n",
      "100/100 [==============================] - 27s 269ms/step - loss: 1.6985 - actual_acc: 0.5162 - val_loss: 2.0283 - val_actual_acc: 0.3358\n",
      "Epoch 405/500\n",
      "100/100 [==============================] - 26s 261ms/step - loss: 1.4535 - actual_acc: 0.5625 - val_loss: 2.1448 - val_actual_acc: 0.3052\n",
      "Epoch 406/500\n",
      "100/100 [==============================] - 18s 177ms/step - loss: 1.2682 - actual_acc: 0.6300 - val_loss: 2.1934 - val_actual_acc: 0.2718\n",
      "Epoch 407/500\n",
      "100/100 [==============================] - 26s 256ms/step - loss: 1.0737 - actual_acc: 0.6688 - val_loss: 2.0877 - val_actual_acc: 0.2972\n",
      "Epoch 408/500\n",
      "100/100 [==============================] - 23s 230ms/step - loss: 1.7928 - actual_acc: 0.4862 - val_loss: 2.3278 - val_actual_acc: 0.2856\n",
      "Epoch 409/500\n",
      "100/100 [==============================] - 26s 259ms/step - loss: 1.4073 - actual_acc: 0.5656 - val_loss: 2.4392 - val_actual_acc: 0.2689\n",
      "Epoch 410/500\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 1.1040 - actual_acc: 0.6637 - val_loss: 2.5422 - val_actual_acc: 0.2507\n",
      "Epoch 411/500\n",
      "100/100 [==============================] - 30s 295ms/step - loss: 1.8339 - actual_acc: 0.4662 - val_loss: 2.6641 - val_actual_acc: 0.2435\n",
      "Epoch 412/500\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 1.5691 - actual_acc: 0.5294 - val_loss: 2.3490 - val_actual_acc: 0.2733\n",
      "Epoch 413/500\n",
      "100/100 [==============================] - 26s 263ms/step - loss: 1.2688 - actual_acc: 0.6213 - val_loss: 2.2863 - val_actual_acc: 0.2733\n",
      "Epoch 414/500\n",
      "100/100 [==============================] - 24s 240ms/step - loss: 1.4724 - actual_acc: 0.5731 - val_loss: 2.8914 - val_actual_acc: 0.1969\n",
      "Epoch 415/500\n",
      "100/100 [==============================] - 24s 239ms/step - loss: 1.7841 - actual_acc: 0.4612 - val_loss: 2.7673 - val_actual_acc: 0.2369\n",
      "Epoch 416/500\n",
      "100/100 [==============================] - 18s 178ms/step - loss: 1.5098 - actual_acc: 0.5406 - val_loss: 2.8758 - val_actual_acc: 0.2318\n",
      "Epoch 417/500\n",
      "100/100 [==============================] - 22s 221ms/step - loss: 1.2357 - actual_acc: 0.6213 - val_loss: 2.1533 - val_actual_acc: 0.2900\n",
      "Epoch 418/500\n",
      "100/100 [==============================] - 25s 254ms/step - loss: 1.7486 - actual_acc: 0.5100 - val_loss: 2.4444 - val_actual_acc: 0.2558\n",
      "Epoch 419/500\n",
      "100/100 [==============================] - 20s 203ms/step - loss: 1.6577 - actual_acc: 0.5106 - val_loss: 2.5816 - val_actual_acc: 0.2238\n",
      "Epoch 420/500\n",
      "100/100 [==============================] - 18s 180ms/step - loss: 1.3150 - actual_acc: 0.6094 - val_loss: 2.1715 - val_actual_acc: 0.2762\n",
      "Epoch 421/500\n",
      "100/100 [==============================] - 25s 252ms/step - loss: 1.1102 - actual_acc: 0.6644 - val_loss: 2.4228 - val_actual_acc: 0.2267\n",
      "Epoch 422/500\n",
      "100/100 [==============================] - 25s 245ms/step - loss: 1.5590 - actual_acc: 0.5463 - val_loss: 1.8976 - val_actual_acc: 0.3016\n",
      "Epoch 423/500\n",
      "100/100 [==============================] - 25s 251ms/step - loss: 1.7571 - actual_acc: 0.4800 - val_loss: 2.2150 - val_actual_acc: 0.2842\n",
      "Epoch 424/500\n",
      "100/100 [==============================] - 18s 178ms/step - loss: 1.4083 - actual_acc: 0.5737 - val_loss: 2.1765 - val_actual_acc: 0.2776\n",
      "Epoch 425/500\n",
      "100/100 [==============================] - 22s 219ms/step - loss: 1.1863 - actual_acc: 0.6406 - val_loss: 1.9490 - val_actual_acc: 0.3561\n",
      "Epoch 426/500\n",
      "100/100 [==============================] - 26s 264ms/step - loss: 1.6404 - actual_acc: 0.5250 - val_loss: 2.0950 - val_actual_acc: 0.2740\n",
      "Epoch 427/500\n",
      "100/100 [==============================] - 21s 213ms/step - loss: 1.7014 - actual_acc: 0.5150 - val_loss: 2.2699 - val_actual_acc: 0.2871\n",
      "Epoch 428/500\n",
      "100/100 [==============================] - 18s 180ms/step - loss: 1.3572 - actual_acc: 0.5956 - val_loss: 2.3178 - val_actual_acc: 0.2922\n",
      "Epoch 429/500\n",
      "100/100 [==============================] - 22s 221ms/step - loss: 1.1162 - actual_acc: 0.6631 - val_loss: 2.4609 - val_actual_acc: 0.2129\n",
      "Epoch 430/500\n",
      "100/100 [==============================] - 32s 316ms/step - loss: 1.7163 - actual_acc: 0.5069 - val_loss: 2.2017 - val_actual_acc: 0.2544\n",
      "Epoch 431/500\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 1.5581 - actual_acc: 0.5394 - val_loss: 1.9079 - val_actual_acc: 0.3634\n",
      "Epoch 432/500\n",
      "100/100 [==============================] - 26s 257ms/step - loss: 1.3145 - actual_acc: 0.5850 - val_loss: 2.0291 - val_actual_acc: 0.3103\n",
      "Epoch 433/500\n",
      "100/100 [==============================] - 17s 173ms/step - loss: 1.0642 - actual_acc: 0.6625 - val_loss: 1.9582 - val_actual_acc: 0.3408\n",
      "Epoch 434/500\n",
      "100/100 [==============================] - 38s 381ms/step - loss: 1.7779 - actual_acc: 0.4856 - val_loss: 2.1900 - val_actual_acc: 0.3270\n",
      "Epoch 435/500\n",
      "100/100 [==============================] - 18s 179ms/step - loss: 1.3869 - actual_acc: 0.5969 - val_loss: 2.0868 - val_actual_acc: 0.3343\n",
      "Epoch 436/500\n",
      "100/100 [==============================] - 42s 425ms/step - loss: 1.1476 - actual_acc: 0.6562 - val_loss: 2.0149 - val_actual_acc: 0.3096\n",
      "Epoch 437/500\n",
      "100/100 [==============================] - 18s 180ms/step - loss: 0.9476 - actual_acc: 0.7119 - val_loss: 2.2800 - val_actual_acc: 0.2929\n",
      "Epoch 438/500\n",
      "100/100 [==============================] - 56s 562ms/step - loss: 1.8463 - actual_acc: 0.4794 - val_loss: 2.5000 - val_actual_acc: 0.2565\n",
      "Epoch 439/500\n",
      "100/100 [==============================] - 18s 177ms/step - loss: 1.5580 - actual_acc: 0.5356 - val_loss: 2.3800 - val_actual_acc: 0.2631\n",
      "Epoch 440/500\n",
      "100/100 [==============================] - 20s 205ms/step - loss: 1.2622 - actual_acc: 0.6125 - val_loss: 2.1250 - val_actual_acc: 0.3161\n",
      "Epoch 441/500\n",
      "100/100 [==============================] - 30s 299ms/step - loss: 1.0744 - actual_acc: 0.6819 - val_loss: 2.1709 - val_actual_acc: 0.2994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 442/500\n",
      "100/100 [==============================] - 26s 261ms/step - loss: 1.5068 - actual_acc: 0.5650 - val_loss: 2.2471 - val_actual_acc: 0.3045\n",
      "Epoch 443/500\n",
      "100/100 [==============================] - 80s 804ms/step - loss: 1.8516 - actual_acc: 0.4838 - val_loss: 2.2969 - val_actual_acc: 0.2943\n",
      "Epoch 444/500\n",
      "100/100 [==============================] - 18s 178ms/step - loss: 1.5399 - actual_acc: 0.5444 - val_loss: 2.0894 - val_actual_acc: 0.3147\n",
      "Epoch 445/500\n",
      "100/100 [==============================] - 53s 531ms/step - loss: 1.2731 - actual_acc: 0.6262 - val_loss: 2.0621 - val_actual_acc: 0.3045\n",
      "Epoch 446/500\n",
      "100/100 [==============================] - 18s 179ms/step - loss: 1.0742 - actual_acc: 0.6819 - val_loss: 2.1822 - val_actual_acc: 0.2900\n",
      "Epoch 447/500\n",
      "100/100 [==============================] - 56s 558ms/step - loss: 1.6307 - actual_acc: 0.5344 - val_loss: 2.2762 - val_actual_acc: 0.2609\n",
      "Epoch 448/500\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 1.5604 - actual_acc: 0.5125 - val_loss: 2.2420 - val_actual_acc: 0.2558\n",
      "Epoch 449/500\n",
      "100/100 [==============================] - 31s 313ms/step - loss: 1.2766 - actual_acc: 0.6112 - val_loss: 2.2679 - val_actual_acc: 0.2667\n",
      "Epoch 450/500\n",
      "100/100 [==============================] - 18s 179ms/step - loss: 1.1185 - actual_acc: 0.6650 - val_loss: 2.1511 - val_actual_acc: 0.2936\n",
      "Epoch 451/500\n",
      "100/100 [==============================] - 45s 451ms/step - loss: 1.9720 - actual_acc: 0.4469 - val_loss: 2.5089 - val_actual_acc: 0.2376\n",
      "Epoch 452/500\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 1.5051 - actual_acc: 0.5519 - val_loss: 2.3989 - val_actual_acc: 0.2362\n",
      "Epoch 453/500\n",
      "100/100 [==============================] - 26s 265ms/step - loss: 1.1839 - actual_acc: 0.6469 - val_loss: 2.7739 - val_actual_acc: 0.2173\n",
      "Epoch 454/500\n",
      "100/100 [==============================] - 18s 177ms/step - loss: 1.0155 - actual_acc: 0.6800 - val_loss: 2.4715 - val_actual_acc: 0.2565\n",
      "Epoch 455/500\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 1.8524 - actual_acc: 0.4919 - val_loss: 2.0235 - val_actual_acc: 0.3430\n",
      "Epoch 456/500\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 1.6100 - actual_acc: 0.5069 - val_loss: 2.0715 - val_actual_acc: 0.3263\n",
      "Epoch 457/500\n",
      "100/100 [==============================] - 32s 318ms/step - loss: 1.2757 - actual_acc: 0.6306 - val_loss: 2.1751 - val_actual_acc: 0.3336\n",
      "Epoch 458/500\n",
      "100/100 [==============================] - 18s 179ms/step - loss: 1.0707 - actual_acc: 0.6706 - val_loss: 2.2601 - val_actual_acc: 0.2711\n",
      "Epoch 459/500\n",
      "100/100 [==============================] - 80s 802ms/step - loss: 1.8877 - actual_acc: 0.4738 - val_loss: 2.3243 - val_actual_acc: 0.3045\n",
      "Epoch 460/500\n",
      "100/100 [==============================] - 19s 187ms/step - loss: 1.3466 - actual_acc: 0.5975 - val_loss: 2.5787 - val_actual_acc: 0.2645\n",
      "Epoch 461/500\n",
      "100/100 [==============================] - 19s 188ms/step - loss: 1.0965 - actual_acc: 0.6725 - val_loss: 2.2465 - val_actual_acc: 0.2929\n",
      "Epoch 462/500\n",
      "100/100 [==============================] - 21s 211ms/step - loss: 0.8878 - actual_acc: 0.7169 - val_loss: 2.3324 - val_actual_acc: 0.2863\n",
      "Epoch 463/500\n",
      "100/100 [==============================] - 23s 225ms/step - loss: 1.9354 - actual_acc: 0.4419 - val_loss: 2.0993 - val_actual_acc: 0.2965\n",
      "Epoch 464/500\n",
      "100/100 [==============================] - 18s 178ms/step - loss: 1.4319 - actual_acc: 0.5588 - val_loss: 2.0875 - val_actual_acc: 0.2747\n",
      "Epoch 465/500\n",
      "100/100 [==============================] - 23s 229ms/step - loss: 1.1130 - actual_acc: 0.6538 - val_loss: 1.9501 - val_actual_acc: 0.3830\n",
      "Epoch 466/500\n",
      "100/100 [==============================] - 30s 304ms/step - loss: 1.8885 - actual_acc: 0.4431 - val_loss: 2.1427 - val_actual_acc: 0.3176\n",
      "Epoch 467/500\n",
      "100/100 [==============================] - 18s 177ms/step - loss: 1.4416 - actual_acc: 0.5594 - val_loss: 1.9949 - val_actual_acc: 0.2885\n",
      "Epoch 468/500\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 1.1750 - actual_acc: 0.6206 - val_loss: 1.9987 - val_actual_acc: 0.3023\n",
      "Epoch 469/500\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 1.2216 - actual_acc: 0.6606 - val_loss: 2.0803 - val_actual_acc: 0.3408\n",
      "Epoch 470/500\n",
      "100/100 [==============================] - 25s 253ms/step - loss: 1.7578 - actual_acc: 0.4844 - val_loss: 2.0329 - val_actual_acc: 0.3292\n",
      "Epoch 471/500\n",
      "100/100 [==============================] - 18s 177ms/step - loss: 1.3515 - actual_acc: 0.5975 - val_loss: 2.1590 - val_actual_acc: 0.2674\n",
      "Epoch 472/500\n",
      "100/100 [==============================] - 27s 272ms/step - loss: 1.0543 - actual_acc: 0.6737 - val_loss: 2.0426 - val_actual_acc: 0.3249\n",
      "Epoch 473/500\n",
      "100/100 [==============================] - 23s 230ms/step - loss: 1.1919 - actual_acc: 0.6456 - val_loss: 2.0488 - val_actual_acc: 0.3132\n",
      "Epoch 474/500\n",
      "100/100 [==============================] - 25s 254ms/step - loss: 1.6014 - actual_acc: 0.5413 - val_loss: 2.0667 - val_actual_acc: 0.3169\n",
      "Epoch 475/500\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 1.1287 - actual_acc: 0.6569 - val_loss: 2.0361 - val_actual_acc: 0.3292\n",
      "Epoch 476/500\n",
      "100/100 [==============================] - 34s 336ms/step - loss: 1.3527 - actual_acc: 0.6112 - val_loss: 2.2975 - val_actual_acc: 0.2849\n",
      "Epoch 477/500\n",
      "100/100 [==============================] - 17s 173ms/step - loss: 1.6538 - actual_acc: 0.5175 - val_loss: 2.3450 - val_actual_acc: 0.2754\n",
      "Epoch 478/500\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 1.2981 - actual_acc: 0.5881 - val_loss: 2.0837 - val_actual_acc: 0.3176\n",
      "Epoch 479/500\n",
      "100/100 [==============================] - 18s 178ms/step - loss: 1.1189 - actual_acc: 0.6506 - val_loss: 2.1022 - val_actual_acc: 0.2965\n",
      "Epoch 480/500\n",
      "100/100 [==============================] - 34s 339ms/step - loss: 1.5453 - actual_acc: 0.5588 - val_loss: 2.2243 - val_actual_acc: 0.2776\n",
      "Epoch 481/500\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 1.7718 - actual_acc: 0.4731 - val_loss: 2.3565 - val_actual_acc: 0.2369\n",
      "Epoch 482/500\n",
      "100/100 [==============================] - 26s 255ms/step - loss: 1.3915 - actual_acc: 0.5719 - val_loss: 2.2451 - val_actual_acc: 0.3038\n",
      "Epoch 483/500\n",
      "100/100 [==============================] - 18s 177ms/step - loss: 1.2109 - actual_acc: 0.6188 - val_loss: 2.1116 - val_actual_acc: 0.3154\n",
      "Epoch 484/500\n",
      "100/100 [==============================] - 34s 343ms/step - loss: 1.5696 - actual_acc: 0.5319 - val_loss: 2.2256 - val_actual_acc: 0.2820\n",
      "Epoch 485/500\n",
      "100/100 [==============================] - 17s 175ms/step - loss: 1.2972 - actual_acc: 0.6156 - val_loss: 2.3300 - val_actual_acc: 0.3067\n",
      "Epoch 486/500\n",
      "100/100 [==============================] - 21s 214ms/step - loss: 1.0630 - actual_acc: 0.6850 - val_loss: 2.5721 - val_actual_acc: 0.2740\n",
      "Epoch 487/500\n",
      "100/100 [==============================] - 18s 178ms/step - loss: 0.8617 - actual_acc: 0.7350 - val_loss: 2.2101 - val_actual_acc: 0.3249\n",
      "Epoch 488/500\n",
      "100/100 [==============================] - 33s 327ms/step - loss: 1.5505 - actual_acc: 0.5725 - val_loss: 2.1494 - val_actual_acc: 0.2769\n",
      "Epoch 489/500\n",
      "100/100 [==============================] - 17s 175ms/step - loss: 1.5725 - actual_acc: 0.5363 - val_loss: 2.0415 - val_actual_acc: 0.3372\n",
      "Epoch 490/500\n",
      "100/100 [==============================] - 20s 204ms/step - loss: 1.1877 - actual_acc: 0.6262 - val_loss: 2.2415 - val_actual_acc: 0.3067\n",
      "Epoch 491/500\n",
      "100/100 [==============================] - 18s 180ms/step - loss: 1.0649 - actual_acc: 0.6781 - val_loss: 2.4206 - val_actual_acc: 0.2849\n",
      "Epoch 492/500\n",
      "100/100 [==============================] - 24s 241ms/step - loss: 1.2543 - actual_acc: 0.6213 - val_loss: 2.2701 - val_actual_acc: 0.3154\n",
      "Epoch 493/500\n",
      "100/100 [==============================] - 23s 232ms/step - loss: 1.8466 - actual_acc: 0.4644 - val_loss: 2.1990 - val_actual_acc: 0.3031\n",
      "Epoch 494/500\n",
      "100/100 [==============================] - 22s 224ms/step - loss: 1.5072 - actual_acc: 0.5300 - val_loss: 2.0737 - val_actual_acc: 0.3394\n",
      "Epoch 495/500\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 1.2312 - actual_acc: 0.6231 - val_loss: 2.1330 - val_actual_acc: 0.3227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 496/500\n",
      "100/100 [==============================] - 30s 303ms/step - loss: 1.4227 - actual_acc: 0.5913 - val_loss: 2.3695 - val_actual_acc: 0.2580\n",
      "Epoch 497/500\n",
      "100/100 [==============================] - 18s 177ms/step - loss: 1.7519 - actual_acc: 0.4875 - val_loss: 2.1140 - val_actual_acc: 0.2929\n",
      "Epoch 498/500\n",
      "100/100 [==============================] - 21s 208ms/step - loss: 1.3881 - actual_acc: 0.5844 - val_loss: 2.0953 - val_actual_acc: 0.2631\n",
      "Epoch 499/500\n",
      "100/100 [==============================] - 27s 266ms/step - loss: 1.1225 - actual_acc: 0.6669 - val_loss: 2.0712 - val_actual_acc: 0.2980\n",
      "Epoch 500/500\n",
      "100/100 [==============================] - 26s 258ms/step - loss: 1.3161 - actual_acc: 0.6269 - val_loss: 2.1034 - val_actual_acc: 0.3212\n"
     ]
    }
   ],
   "source": [
    "if not use_rotation:\n",
    "    class_weight = {0: 1.15,\n",
    "                     1: 1.13,\n",
    "                     2: 1.27,\n",
    "                     3: 1.51,\n",
    "                     4: 1.46,\n",
    "                     5: 1.19,\n",
    "                     6: 1.0,\n",
    "                     7: 1.04,\n",
    "                     8: 1.11,\n",
    "                     9: 1.2,\n",
    "                     10: 1.27,\n",
    "                     11: 1.34}\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir='log_dir\\\\' + model_name\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=model_name,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "    )\n",
    "    \n",
    "]\n",
    "'''keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='loss', \n",
    "        factor=0.5,                              \n",
    "        patience=70, \n",
    "        min_lr=0.00001\n",
    "    )'''\n",
    "\n",
    "steps_per_epoch = 100\n",
    "history = model.fit_generator(train_generator,\n",
    "                            steps_per_epoch=steps_per_epoch,\n",
    "                            epochs=500,\n",
    "                            validation_data=validation_generator,\n",
    "                            validation_steps=int(round(steps_per_epoch/data_ratio*(1-data_ratio))),\n",
    "                            shuffle=True,\n",
    "                            class_weight=class_weight,\n",
    "                            callbacks=callbacks,\n",
    "                            initial_epoch=initial_epoch\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "    1)   \n",
    "    2) baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
